{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06b0b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a55b0c",
   "metadata": {},
   "source": [
    "# Finding Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfb6b3b",
   "metadata": {},
   "source": [
    "## Defining Model Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec456bd",
   "metadata": {},
   "source": [
    "### Pruning vs Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305d3738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(p):\n",
    "    #6th order polynomial coefficients\n",
    "    coeffs = [85.28, -10.35, -28.44, -49.40]\n",
    "    \n",
    "    acc = sum(c * p**i for i, c in enumerate(coeffs))\n",
    "\n",
    "    return max(acc, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9005933b",
   "metadata": {},
   "source": [
    "### Pruning vs Model Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbda8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size(p):\n",
    "    coeffs = [496.7, -706.2, 276.9, 4.020]\n",
    "    \n",
    "    return sum(c * p**i for i, c in enumerate(coeffs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fd7d8f",
   "metadata": {},
   "source": [
    "### Pruning vs Inference Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3498c006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time(p):\n",
    "    b0 = 0.06386\n",
    "    b1 = -0.06386\n",
    "    \n",
    "    return (b0 + b1 * p)*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbfedfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.1\n",
    "get_accuracy(p), get_size(p), get_time(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47436e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(arr):\n",
    "    min_element = min(arr)\n",
    "    max_element = max(arr)\n",
    "    return [(x - min_element) / (max_element - min_element) for x in arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5347dc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_amounts = np.arange(0.0, 1.001, 0.001)\n",
    "plt.figure(figsize=(10, 6))\n",
    "accuracies = [get_accuracy(p) for p in pruning_amounts]\n",
    "sizes = [get_size(p) for p in pruning_amounts]\n",
    "times = [get_time(p) for p in pruning_amounts]\n",
    "\n",
    "plt.plot(pruning_amounts, normalize(accuracies), label='Accuracy')\n",
    "plt.plot(pruning_amounts, normalize(sizes), label='Size')\n",
    "plt.plot(pruning_amounts, normalize(times), label='Time')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408e9525",
   "metadata": {},
   "source": [
    "#$ Defining Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969bbbb1",
   "metadata": {},
   "source": [
    "### Accuracy Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a004812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_reward(curr_accuracy, min_accuracy, sigma_right=4, sigma_left=2):\n",
    "    diff = curr_accuracy - min_accuracy\n",
    "    if 0<=diff<=1e-2:\n",
    "        return (np.exp(- (diff**2) / (10 * sigma_left**2)) * 100)\n",
    "    else:\n",
    "        return 1*(np.exp(- (abs(diff)**1.5) / (2 * sigma_right**2)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532e4588",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_accuracy = float(input(\"Enter the minimum acceptable accuracy: \") or 80.0)\n",
    "acc_rewards = []\n",
    "accuracies = []\n",
    "pruning_amounts = np.arange(0.0, 1.001, 0.001)\n",
    "for p in pruning_amounts:\n",
    "    accuracy = get_accuracy(p)\n",
    "    reward = get_accuracy_reward(accuracy, min_accuracy=80)\n",
    "    accuracies.append(accuracy)\n",
    "    acc_rewards.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250bd397",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "plt.figure(figsize=(15, 6), dpi=100)\n",
    "plt.plot(pruning_amounts, acc_rewards, label='Accuracy Reward', color='blue')\n",
    "plt.plot(pruning_amounts, accuracies, label='Accuracy', color='red')\n",
    "plt.ylim(0)\n",
    "plt.axhline(y=80, color='black', linestyle='--')\n",
    "plt.xlabel('Pruning Amount')\n",
    "plt.ylabel('Reward or Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94ac797",
   "metadata": {},
   "source": [
    "### Inference Time Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e17925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comp_time_reward(current_comp_time, sigma=0.8):\n",
    "    return np.exp(- (current_comp_time**2) / (2 * sigma**2))*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d2076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_rewards = []\n",
    "comp_times = []\n",
    "for p in pruning_amounts:\n",
    "    time = get_time(p)\n",
    "    reward = get_comp_time_reward(time)\n",
    "    comp_times.append(time)\n",
    "    time_rewards.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165654a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "plt.figure(figsize=(15, 6), dpi=100)\n",
    "plt.plot(pruning_amounts, time_rewards, label='Time Reward', color='blue')\n",
    "plt.plot(pruning_amounts, [x * 100 for x in comp_times], label='Comp Time', color='red')\n",
    "plt.ylim(0)\n",
    "plt.axhline(y=0, color='black', linestyle='--')\n",
    "plt.xlabel('Pruning Amount')\n",
    "plt.ylabel('Reward or Comp Time')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae028a7",
   "metadata": {},
   "source": [
    "### Model Size Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563b6212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size_reward(current_model_size, max_model_size, sigma_left=2):\n",
    "    diff = current_model_size - max_model_size\n",
    "    if current_model_size > max_model_size:\n",
    "        return np.exp(- ((diff)**2) / (10 * sigma_left**2))*99*0.5\n",
    "    if current_model_size == max_model_size:\n",
    "        return 99*(0.5)\n",
    "    else:\n",
    "        return (99+(current_model_size/max_model_size))*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a0e8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_rewards = []\n",
    "sizes = []\n",
    "# max_model_size = float(input(\"Enter the maximum acceptable model size: \") or 300.0)\n",
    "for p in pruning_amounts:\n",
    "    size = get_size(p)\n",
    "    reward = get_model_size_reward(size, max_model_size=300)\n",
    "    sizes.append(size)\n",
    "    size_rewards.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588711e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "plt.figure(figsize=(15, 6), dpi=100)\n",
    "plt.plot(pruning_amounts, size_rewards, label='Size Reward', color='blue')\n",
    "plt.plot(pruning_amounts, sizes, label='Size', color='red')\n",
    "plt.ylim(0)\n",
    "plt.xlabel('Pruning Amount')\n",
    "plt.ylabel('Size or Size Reward')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f982b2b",
   "metadata": {},
   "source": [
    "### Reward for better pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fcb8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def more_acc_less_size(accuracy, min_accuracy, size, max_model_size):\n",
    "    if accuracy >= min_accuracy and size <= max_model_size:\n",
    "        return ((accuracy-min_accuracy)*2) + (max_model_size-size)/2\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57608426",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_of_calulating_rewards = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65783e1a",
   "metadata": {},
   "source": [
    "### Final Reward Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fd47f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(p, min_accuracy=80.0, max_model_size=350.0, x=10, y=1, z=1) -> float:\n",
    "    accuracy = get_accuracy(p)\n",
    "    time = get_time(p)\n",
    "    size = get_size(p)\n",
    "    \n",
    "    acc_reward = np.array(get_accuracy_reward(accuracy, min_accuracy))\n",
    "    time_reward = np.array(get_comp_time_reward(time))\n",
    "    size_reward = np.array(get_model_size_reward(size, max_model_size))\n",
    "    better_reward = more_acc_less_size(accuracy, min_accuracy, size, max_model_size)\n",
    "    global counts_of_calulating_rewards\n",
    "    counts_of_calulating_rewards += 1 # type: ignore\n",
    "    \n",
    "    x, y, z = x/(x+y+z), y/(x+y+z), z/(x+y+z)\n",
    "    \n",
    "    return (x*acc_reward + y*time_reward + z*size_reward + better_reward + p/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98bacfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_details(p,min_accuracy, max_model_size):\n",
    "    return {\n",
    "        'Min Accuracy Required': min_accuracy,\n",
    "        'Max Model Size Required': max_model_size,\n",
    "        'Pruning Amount': p,\n",
    "        \"Accuracy\": get_accuracy(p),\n",
    "        \"Time\": get_time(p),\n",
    "        \"Size\": get_size(p),\n",
    "        \"Accuracy Reward\": get_accuracy_reward(get_accuracy(p), min_accuracy),\n",
    "        \"Time Reward\": get_comp_time_reward(get_time(p)),\n",
    "        \"Size Reward\": get_model_size_reward(get_size(p), max_model_size),\n",
    "        \"More Acc Less Size Reward\": more_acc_less_size(get_accuracy(p), min_accuracy, get_size(p), max_model_size),\n",
    "        \"Pruning Reward\": p * 100,\n",
    "        \"Total Reward\": get_reward(p),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d72c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_amounts = np.arange(0.0, 1.001, 0.001)\n",
    "min_accuracy = 80.0\n",
    "max_model_size = 300.0\n",
    "results = [get_details(p, min_accuracy, max_model_size) for p in pruning_amounts]\n",
    "\n",
    "plt.figure(figsize=(15, 6), dpi=100)\n",
    "plt.plot(pruning_amounts, [r['Accuracy Reward'] for r in results], label='Accuracy', color='blue')\n",
    "plt.plot(pruning_amounts, [r['Size Reward'] for r in results], label='Size', color='green')\n",
    "plt.plot(pruning_amounts, [r['More Acc Less Size Reward'] for r in results], label='More Acc Less Size', color='orange')\n",
    "plt.plot(pruning_amounts, [r['Pruning Reward'] for r in results], label='Pruning Reward', color='purple')\n",
    "plt.plot(pruning_amounts, [r['Time Reward'] for r in results], label='Time Reward', color='black')\n",
    "plt.plot(pruning_amounts, [r['Total Reward'] for r in results], label='Total Reward', color='red', linewidth=3)\n",
    "plt.xlabel('Pruning Amount')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ce15ae",
   "metadata": {},
   "source": [
    "## Global Optimization with CMA-ES\n",
    "Here we apply the CMA-ES evolutionary strategy for robust, gradient-free maximization of the total reward over the pruning amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e3ea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_pruning_amount(min_accuracy=80.0, max_model_size=300.0):\n",
    "    def objective(x):\n",
    "        p = x[0]\n",
    "        return -1 * get_reward(p, min_accuracy=min_accuracy, max_model_size=max_model_size) \n",
    "\n",
    "    x0 = [0.1, 0.0]\n",
    "    sigma = 0.2\n",
    "    bounds = [[0.0, -float('inf')], [1.0, float('inf')]]\n",
    "\n",
    "    es = cma.CMAEvolutionStrategy(x0, sigma, {\n",
    "        'bounds': bounds,\n",
    "        'popsize': 20,\n",
    "        'CMA_diagonal': 0\n",
    "    })\n",
    "\n",
    "    for _ in range(100):\n",
    "        candidates = es.ask()\n",
    "        fitnesses = [objective(c) for c in candidates]\n",
    "        es.tell(candidates, fitnesses)\n",
    "\n",
    "    opt_p = es.result.xbest[0]\n",
    "    # print(f\"CMA-ES found optimal pruning amount: {opt_p:.4f}\")\n",
    "    # print(f\"Maximum reward: {-es.result.fbest:.4f}\")\n",
    "    # print(f\"Number of reward calculations: {counts_of_calulating_rewards}\") # type: ignore\n",
    "    return opt_p.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96425b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = [get_reward(p) for p in pruning_amounts]\n",
    "max_index = np.argmax(rewards)\n",
    "max_reward = rewards[max_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301c3e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_accuracy = float(input(\"Enter the minimum acceptable accuracy: \") or 80.0)\n",
    "max_model_size = float(input(\"Enter the maximum acceptable model size: \") or 300.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b68cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_p = get_best_pruning_amount(min_accuracy=min_accuracy, max_model_size=max_model_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cf8b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(opt_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec07466",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_details(opt_p, min_accuracy=min_accuracy, max_model_size=max_model_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d96aef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results to compare brute-force and CMA-ES\n",
    "plt.style.use('default')\n",
    "plt.figure(figsize=(15, 6), dpi=100)\n",
    "plt.plot(pruning_amounts, rewards, label='Sum of all Rewards', color='green')\n",
    "\n",
    "# Brute-force result\n",
    "brute_force_p = pruning_amounts[max_index]\n",
    "plt.axvline(x=brute_force_p, color='red', linestyle='-', label=f'Brute-force max: {brute_force_p:.3f}')\n",
    "\n",
    "# CMA-ES result\n",
    "plt.axvline(x=opt_p, color='purple', linestyle='--', label=f'CMA-ES max: {opt_p:.3f}')\n",
    "\n",
    "plt.ylim(0)\n",
    "plt.xlabel('Pruning Amount')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Total Reward vs. Pruning Amount')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Brute-force search optimal pruning amount: {brute_force_p:.4f}\")\n",
    "print(f\"CMA-ES found optimal pruning amount: {opt_p:.4f}\")\n",
    "\n",
    "# Also print the rewards at these points\n",
    "reward_brute_force = get_reward(brute_force_p)\n",
    "reward_cma_es = get_reward(opt_p)\n",
    "\n",
    "print(f\"Reward at brute-force optimum: {reward_brute_force:.2f}\")\n",
    "print(f\"Reward at CMA-ES optimum: {reward_cma_es:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b211b157",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_details(brute_force_p, min_accuracy=min_accuracy, max_model_size=max_model_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a1a276",
   "metadata": {},
   "source": [
    "# Finding Initial Importance of Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51074983",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_single_view = np.array(\n",
    "    [18.15, 12.55, 8.79, 10.09, 11.37, 14.52, 10.85, 12.94, 15.52, 9.79, 8.49, 7.62])\n",
    "\n",
    "delta_drop_view = np.array(\n",
    "    [0.32, 0.16, 0.63, 0.57, 0.32, 0.07, 0.22, 0.07, 0.51, 0.87, 1.17, 0.78])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7d4ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, 13), delta_single_view, marker='o', color='blue', label='Delta Single View')\n",
    "plt.plot(range(1, 13), delta_drop_view, marker='o', color='red', label='Delta Drop View')\n",
    "plt.xlabel('View Number')\n",
    "plt.ylabel('Delta Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe930a7",
   "metadata": {},
   "source": [
    "$$ i \\propto acc_{drop} $$\n",
    "$$ \\text{and} $$\n",
    "$$ i \\propto \\frac {1} {acc_{view}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42d1642",
   "metadata": {},
   "source": [
    "$$ i \\propto \\frac {acc_{drop}} {acc_{view}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959a5508",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = (delta_drop_view/delta_single_view)/np.sum(delta_drop_view/delta_single_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b090ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0, 12), i, marker='x', color='orange', label='Importance Index')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83d0b60",
   "metadata": {},
   "source": [
    "# Per View Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4d0b1d",
   "metadata": {},
   "source": [
    "Assuming $acc_f = \\sum_{v=0}^v acc_m \\cdot i_v$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf38784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_model_accuracy = 85.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e89e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_view_acc_req = whole_model_accuracy*i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76920e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_view_acc_req, sum(per_view_acc_req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae84890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pruning_from_accuracy(target_acc):\n",
    "    \"\"\"Inverse function to find pruning amount for a target accuracy\"\"\"\n",
    "    # Using Newton-Raphson method to solve for p given target accuracy\n",
    "    p = 0.1  # initial guess\n",
    "    for _ in range(100):  # max iterations\n",
    "        current_acc = get_accuracy(p)\n",
    "        if abs(current_acc - target_acc) < 1e-6:\n",
    "            break\n",
    "        \n",
    "        # Calculate derivative numerically\n",
    "        eps = 1e-8\n",
    "        acc_plus = get_accuracy(p + eps)\n",
    "        acc_minus = get_accuracy(p - eps)\n",
    "        derivative = (acc_plus - acc_minus) / (2 * eps)\n",
    "        \n",
    "        if abs(derivative) < 1e-10:  # avoid division by zero\n",
    "            break\n",
    "            \n",
    "        # Newton-Raphson update\n",
    "        p = p - (current_acc - target_acc) / derivative\n",
    "        p = max(0.0, min(1.0, p))  # clamp to valid range\n",
    "    \n",
    "    return p\n",
    "\n",
    "def calculate_per_view_reward(per_view_accuracies, min_accuracy=80.0, max_model_size=300.0):\n",
    "    \"\"\"Calculate total reward for per-view accuracy distribution\"\"\"\n",
    "    total_reward = 0.0\n",
    "    \n",
    "    for view_idx, view_acc in enumerate(per_view_accuracies):\n",
    "        # Ensure minimum accuracy constraint\n",
    "        if view_acc < min_accuracy:\n",
    "            return -1000  # Heavy penalty for violating constraints\n",
    "        \n",
    "        # Calculate pruning amount for this view's target accuracy\n",
    "        p_view = get_pruning_from_accuracy(view_acc)\n",
    "        \n",
    "        # Calculate individual view reward\n",
    "        view_reward = get_reward(p_view, min_accuracy, max_model_size)\n",
    "        total_reward += view_reward * i[view_idx]  # Weight by importance\n",
    "    \n",
    "    return total_reward\n",
    "\n",
    "def optimize_per_view_accuracies(min_accuracy=80.0, max_model_size=300.0, target_total_acc=85.01):\n",
    "    \"\"\"Use CMA-ES to optimize per-view accuracy distribution\"\"\"\n",
    "    \n",
    "    def objective(x):\n",
    "        # x contains 12 per-view accuracies\n",
    "        per_view_accs = np.array(x)\n",
    "        \n",
    "        # Constraint: weighted sum should equal target total accuracy\n",
    "        weighted_sum = np.sum(per_view_accs * i)\n",
    "        constraint_penalty = abs(weighted_sum - target_total_acc) * 100\n",
    "        \n",
    "        # Calculate reward\n",
    "        reward = calculate_per_view_reward(per_view_accs, min_accuracy, max_model_size)\n",
    "        \n",
    "        # Return negative for minimization (CMA-ES minimizes)\n",
    "        return -(reward - constraint_penalty)\n",
    "    \n",
    "    # Initial guess: start with minimum accuracy for all views\n",
    "    x0 = [min_accuracy + 5.0] * 12  # slightly above minimum\n",
    "    sigma = 2.0  # standard deviation for initial search\n",
    "    \n",
    "    # Bounds: format as [lower_bounds, upper_bounds]\n",
    "    lower_bounds = [min_accuracy] * 12\n",
    "    upper_bounds = [100.0] * 12\n",
    "    bounds = [lower_bounds, upper_bounds]\n",
    "    \n",
    "    es = cma.CMAEvolutionStrategy(x0, sigma, {\n",
    "        'bounds': bounds,\n",
    "        'popsize': 50,  # larger population for multi-dimensional problem\n",
    "        'maxiter': 200,\n",
    "        'tolfun': 1e-6\n",
    "    })\n",
    "    \n",
    "    best_solution = None\n",
    "    best_reward = -float('inf')\n",
    "    \n",
    "    while not es.stop():\n",
    "        candidates = es.ask()\n",
    "        fitnesses = [objective(c) for c in candidates]\n",
    "        es.tell(candidates, fitnesses)\n",
    "        \n",
    "        # Track best solution\n",
    "        current_best_idx = np.argmin(fitnesses)\n",
    "        current_best_reward = -fitnesses[current_best_idx]\n",
    "        \n",
    "        if current_best_reward > best_reward:\n",
    "            best_reward = current_best_reward\n",
    "            best_solution = candidates[current_best_idx]\n",
    "    \n",
    "    return np.array(best_solution), best_reward\n",
    "\n",
    "# Run the optimization\n",
    "print(\"Optimizing per-view accuracies using CMA-ES...\")\n",
    "optimal_per_view_accs, optimal_reward = optimize_per_view_accuracies(\n",
    "    min_accuracy=min_accuracy, \n",
    "    max_model_size=max_model_size, \n",
    "    target_total_acc=whole_model_accuracy\n",
    ")\n",
    "\n",
    "print(f\"\\nOptimal per-view accuracies:\")\n",
    "for i_view, acc in enumerate(optimal_per_view_accs):\n",
    "    print(f\"View {i_view + 1}: {acc:.2f}%\")\n",
    "\n",
    "print(f\"\\nWeighted sum accuracy: {np.sum(optimal_per_view_accs * i):.2f}%\")\n",
    "print(f\"Target accuracy: {whole_model_accuracy:.2f}%\")\n",
    "print(f\"Total reward: {optimal_reward:.2f}\")\n",
    "\n",
    "# Calculate corresponding pruning amounts\n",
    "optimal_pruning_amounts = [get_pruning_from_accuracy(acc) for acc in optimal_per_view_accs]\n",
    "print(\"\\nCorresponding pruning amounts:\")\n",
    "for i_view, p in enumerate(optimal_pruning_amounts):\n",
    "    print(f\"View {i_view + 1}: {p:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6017bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Per-view accuracies comparison\n",
    "views = range(1, 13)\n",
    "ax1.bar(views, per_view_acc_req, alpha=0.7, label='Required (proportional)', color='blue')\n",
    "ax1.bar(views, optimal_per_view_accs, alpha=0.7, label='Optimized', color='red')\n",
    "ax1.axhline(y=min_accuracy, color='black', linestyle='--', label=f'Min accuracy ({min_accuracy}%)')\n",
    "ax1.set_xlabel('View Number')\n",
    "ax1.set_ylabel('Accuracy (%)')\n",
    "ax1.set_title('Per-View Accuracy Comparison')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Importance weights\n",
    "ax2.bar(views, i, color='orange', alpha=0.7)\n",
    "ax2.set_xlabel('View Number')\n",
    "ax2.set_ylabel('Importance Weight')\n",
    "ax2.set_title('View Importance Weights')\n",
    "\n",
    "# Plot 3: Pruning amounts per view\n",
    "ax3.bar(views, optimal_pruning_amounts, color='green', alpha=0.7)\n",
    "ax3.set_xlabel('View Number')\n",
    "ax3.set_ylabel('Pruning Amount')\n",
    "ax3.set_title('Optimal Pruning Amount per View')\n",
    "\n",
    "# Plot 4: Individual view rewards\n",
    "individual_rewards = [get_reward(p, min_accuracy, max_model_size) for p in optimal_pruning_amounts]\n",
    "ax4.bar(views, individual_rewards, color='purple', alpha=0.7)\n",
    "ax4.set_xlabel('View Number')\n",
    "ax4.set_ylabel('Individual Reward')\n",
    "ax4.set_title('Reward per View')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nSummary Statistics:\")\n",
    "print(f\"Average per-view accuracy: {np.mean(optimal_per_view_accs):.2f}%\")\n",
    "print(f\"Minimum per-view accuracy: {np.min(optimal_per_view_accs):.2f}%\")\n",
    "print(f\"Maximum per-view accuracy: {np.max(optimal_per_view_accs):.2f}%\")\n",
    "print(f\"Standard deviation: {np.std(optimal_per_view_accs):.2f}%\")\n",
    "print(f\"Average pruning amount: {np.mean(optimal_pruning_amounts):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c05a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_model_sizes = [\n",
    "    200.0,  # View 0\n",
    "    180.0,  # View 1\n",
    "    300.0,  # View 2\n",
    "    250.0,  # View 3\n",
    "    220.0,  # View 4\n",
    "    150.0,  # View 5\n",
    "    200.0,  # View 6\n",
    "    180.0,  # View 7\n",
    "    250.0,  # View 8\n",
    "    300.0,  # View 9\n",
    "    400.0,  # View 10\n",
    "    350.0   # View 11\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e4389c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cma\n",
    "\n",
    "def optimize_target_accuracy_with_individual_constraints(\n",
    "    min_accuracy=80.0,\n",
    "    max_model_sizes=None,\n",
    "    initial_target_acc=85.01\n",
    "):\n",
    "    \"\"\"Use CMA-ES to optimize the target total accuracy that maximizes reward\n",
    "    while satisfying all per-view accuracy & size constraints.\"\"\"\n",
    "    \n",
    "    if max_model_sizes is None:\n",
    "        max_model_sizes = [300.0] * 12\n",
    "    \n",
    "    # Compute lower‐bound so even the smallest-weighted view meets min_accuracy\n",
    "    min_target_from_accuracy = min_accuracy / np.min(i)\n",
    "    if min_target_from_accuracy > 100:\n",
    "        print(f\"Warning: lower‐bound {min_target_from_accuracy:.1f}% > 100%. Relaxing to 80%.\")\n",
    "        min_target_bound = max(initial_target_acc * 0.8, 80.0)\n",
    "    else:\n",
    "        min_target_bound = max(min_target_from_accuracy, initial_target_acc * 0.8)\n",
    "    \n",
    "    upper_bound = min(120.0, initial_target_acc * 1.5)\n",
    "    bounds = [min_target_bound, upper_bound]\n",
    "    print(f\"Searching target_total_acc in [{bounds[0]:.2f}%, {bounds[1]:.2f}%]\")\n",
    "\n",
    "    def objective(x):\n",
    "        T = x[0]  # global target accuracy\n",
    "        per_view_accs = T * i\n",
    "\n",
    "        # penalties for min-accuracy or size violations\n",
    "        total_penalty = 0\n",
    "        for idx, acc in enumerate(per_view_accs):\n",
    "            if acc < min_accuracy:\n",
    "                total_penalty += (min_accuracy - acc)**2 * 1e3\n",
    "            try:\n",
    "                p = get_pruning_from_accuracy(acc)\n",
    "                size = get_size(p)\n",
    "                if size > max_model_sizes[idx]:\n",
    "                    total_penalty += (size - max_model_sizes[idx])**2 * 10\n",
    "            except:\n",
    "                total_penalty += 1e4\n",
    "\n",
    "        # aggregate weighted reward\n",
    "        total_reward = sum(\n",
    "            get_reward(get_pruning_from_accuracy(acc),\n",
    "                       min_accuracy,\n",
    "                       max_model_sizes[idx]) * i[idx]\n",
    "            for idx, acc in enumerate(per_view_accs)\n",
    "        )\n",
    "        return -(total_reward - total_penalty)\n",
    "\n",
    "    # run CMA-ES for us\n",
    "    x0, sigma0 = [initial_target_acc], 5.0\n",
    "    best_sol, best_fit = cma.fmin2(\n",
    "        objective,\n",
    "        x0,\n",
    "        sigma0,\n",
    "        {\n",
    "            'bounds':  bounds,\n",
    "            'popsize': 30,\n",
    "            'maxiter': 200,\n",
    "            'tolfun':  1e-6,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    optimal_T = best_sol[0]\n",
    "    optimal_per_view = optimal_T * i\n",
    "    actual_reward = max(0.0, -best_fit)\n",
    "\n",
    "    return optimal_per_view, optimal_T, actual_reward\n",
    "\n",
    "# Run the corrected optimization\n",
    "print(\"Optimizing target accuracy with proportional per-view accuracy assignment...\")\n",
    "print(f\"Importance weights: {i}\")\n",
    "print(f\"Max model sizes per view: {max_model_sizes}\")\n",
    "print(f\"Required: each view gets accuracy = target_total_acc * importance_weight\")\n",
    "\n",
    "result = optimize_target_accuracy_with_individual_constraints(\n",
    "    min_accuracy=min_accuracy, \n",
    "    max_model_sizes=max_model_sizes, \n",
    "    initial_target_acc=whole_model_accuracy\n",
    ")\n",
    "\n",
    "if result[0] is not None:\n",
    "    optimal_per_view_accs_corrected, optimal_target_acc, optimal_reward_corrected = result\n",
    "    \n",
    "    print(f\"\\nOptimal target total accuracy: {optimal_target_acc:.2f}%\")\n",
    "    print(f\"Optimal per-view accuracies (proportional assignment):\")\n",
    "    for i_view, acc in enumerate(optimal_per_view_accs_corrected):\n",
    "        print(f\"View {i_view + 1}: {acc:.2f}% (= {optimal_target_acc:.2f}% × {i[i_view]:.4f})\")\n",
    "\n",
    "    print(f\"\\nVerification - Sum of weighted accuracies: {np.sum(optimal_per_view_accs_corrected * i):.2f}%\")\n",
    "    print(f\"Should equal target accuracy: {optimal_target_acc:.2f}%\")\n",
    "    print(f\"Total reward: {optimal_reward_corrected:.2f}\")\n",
    "\n",
    "    # Calculate corresponding pruning amounts and model sizes\n",
    "    optimal_pruning_amounts_corrected = [get_pruning_from_accuracy(acc) for acc in optimal_per_view_accs_corrected]\n",
    "    optimal_model_sizes_corrected = [get_size(p) for p in optimal_pruning_amounts_corrected]\n",
    "\n",
    "    print(f\"\\nCorresponding pruning amounts and model sizes:\")\n",
    "    for i_view, (p, size) in enumerate(zip(optimal_pruning_amounts_corrected, optimal_model_sizes_corrected)):\n",
    "        constraint_met = \"✓\" if size <= max_model_sizes[i_view] else \"✗\"\n",
    "        acc_constraint_met = \"✓\" if optimal_per_view_accs_corrected[i_view] >= min_accuracy else \"✗\"\n",
    "        print(f\"View {i_view + 1}: Pruning={p:.4f}, Size={size:.1f} (Max: {max_model_sizes[i_view]:.1f}) {constraint_met}, Acc={optimal_per_view_accs_corrected[i_view]:.1f}% (Min: {min_accuracy}%) {acc_constraint_met}\")\n",
    "else:\n",
    "    print(\"Optimization failed. Using fallback solution with original target accuracy.\")\n",
    "    optimal_target_acc = whole_model_accuracy\n",
    "    optimal_per_view_accs_corrected = optimal_target_acc * i\n",
    "    optimal_reward_corrected = 0\n",
    "    optimal_pruning_amounts_corrected = [get_pruning_from_accuracy(acc) for acc in optimal_per_view_accs_corrected]\n",
    "    optimal_model_sizes_corrected = [get_size(p) for p in optimal_pruning_amounts_corrected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d247a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced visualization comparing both approaches\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "views = range(1, 13)\n",
    "\n",
    "# Plot 1: Per-view accuracies comparison\n",
    "width = 0.25\n",
    "x = np.arange(len(views))\n",
    "\n",
    "ax1.bar(x - width, per_view_acc_req, width, alpha=0.7, label='Required (proportional)', color='blue')\n",
    "ax1.bar(x, optimal_per_view_accs, width, alpha=0.7, label='Optimized (uniform constraints)', color='red')\n",
    "ax1.bar(x + width, optimal_per_view_accs_corrected, width, alpha=0.7, label='Optimized (individual constraints)', color='green')\n",
    "ax1.axhline(y=min_accuracy, color='black', linestyle='--', label=f'Min accuracy ({min_accuracy}%)')\n",
    "ax1.set_xlabel('View Number')\n",
    "ax1.set_ylabel('Accuracy (%)')\n",
    "ax1.set_title('Per-View Accuracy Comparison')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(views)\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Model size constraints vs actual sizes\n",
    "ax2.bar(x - width/2, max_model_sizes, width, alpha=0.7, label='Max allowed size', color='orange')\n",
    "ax2.bar(x + width/2, optimal_model_sizes_corrected, width, alpha=0.7, label='Actual size', color='purple')\n",
    "ax2.set_xlabel('View Number')\n",
    "ax2.set_ylabel('Model Size')\n",
    "ax2.set_title('Model Size Constraints vs Actual Sizes')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(views)\n",
    "ax2.legend()\n",
    "\n",
    "# Plot 3: Pruning amounts comparison\n",
    "ax3.bar(x - width/2, optimal_pruning_amounts, width, alpha=0.7, label='Uniform constraints', color='red')\n",
    "ax3.bar(x + width/2, optimal_pruning_amounts_corrected, width, alpha=0.7, label='Individual constraints', color='green')\n",
    "ax3.set_xlabel('View Number')\n",
    "ax3.set_ylabel('Pruning Amount')\n",
    "ax3.set_title('Optimal Pruning Amount Comparison')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(views)\n",
    "ax3.legend()\n",
    "\n",
    "# Plot 4: Summary metrics\n",
    "metrics = ['Weighted Acc', 'Total Reward', 'Acc Violations', 'Size Violations']\n",
    "acc_perfect_metrics = [solutions[0]['weighted_acc'], solutions[0]['total_reward'], \n",
    "                      solutions[0]['acc_violations'], solutions[0]['size_violations']]\n",
    "size_perfect_metrics = [solutions[1]['weighted_acc'], solutions[1]['total_reward'], \n",
    "                       solutions[1]['acc_violations'], solutions[1]['size_violations']]\n",
    "    \n",
    "x_metrics = np.arange(len(metrics))\n",
    "ax4.bar(x_metrics - width/2, acc_perfect_metrics, width, alpha=0.7, \n",
    "        label='Accuracy-Perfect', color='green')\n",
    "ax4.bar(x_metrics + width/2, size_perfect_metrics, width, alpha=0.7, \n",
    "        label='Size-Perfect', color='blue')\n",
    "ax4.set_xlabel('Metrics')\n",
    "ax4.set_ylabel('Values')\n",
    "ax4.set_title('Solution Metrics Comparison')\n",
    "ax4.set_xticks(x_metrics)\n",
    "ax4.set_xticklabels(metrics)\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary table\n",
    "print(f\"\\n{'Solution':<20} {'Weighted Acc':<12} {'Total Reward':<12} {'Acc Viol':<10} {'Size Viol':<10}\")\n",
    "print(\"-\" * 70)\n",
    "for sol in solutions:\n",
    "    print(f\"{sol['name']:<20} {sol['weighted_acc']:<12.2f} {sol['total_reward']:<12.2f} \"\n",
    "          f\"{sol['acc_violations']:<10} {sol['size_violations']:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8b6570",
   "metadata": {},
   "source": [
    "## Uing NSGA II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef0e9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_model_sizes = [\n",
    "    200.0,  # View 0\n",
    "    180.0,  # View 1\n",
    "    300.0,  # View 2\n",
    "    250.0,  # View 3\n",
    "    220.0,  # View 4\n",
    "    150.0,  # View 5\n",
    "    200.0,  # View 6\n",
    "    180.0,  # View 7\n",
    "    250.0,  # View 8\n",
    "    300.0,  # View 9\n",
    "    400.0,  # View 10\n",
    "    350.0   # View 11\n",
    "]\n",
    "min_accuracy = 80.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb808e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
    "from pymoo.algorithms.soo.nonconvex.ga import GA\n",
    "from pymoo.optimize import minimize\n",
    "from pymoo.core.problem import Problem\n",
    "\n",
    "# Define the corrected multi-view problem\n",
    "class MultiViewProblem(Problem):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            n_var=12,            # one pruning fraction p per view\n",
    "            n_obj=12,            # one objective per view\n",
    "            xl=np.zeros(12),     # pruning fraction p in [0,1]\n",
    "            xu=np.ones(12)\n",
    "        )\n",
    "\n",
    "    def _evaluate(self, X, out, *args, **kwargs):\n",
    "        pop_size, _ = X.shape\n",
    "        F = np.zeros((pop_size, 12))\n",
    "\n",
    "        # Build per-view objectives: minimize -weighted_reward\n",
    "        for j in range(12):\n",
    "            ps = X[:, j]\n",
    "            vals = np.zeros(pop_size)\n",
    "            for k, p in enumerate(ps):\n",
    "                r = get_reward(p, min_accuracy, max_model_sizes[j])\n",
    "                weighted_r = r * i[j]\n",
    "                vals[k] = -weighted_r\n",
    "            F[:, j] = vals\n",
    "\n",
    "        out[\"F\"] = F\n",
    "\n",
    "# Single objective problem for fallback\n",
    "class SingleObjectiveProblem(Problem):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            n_var=12,\n",
    "            n_obj=1,\n",
    "            xl=np.zeros(12),\n",
    "            xu=np.ones(12)\n",
    "        )\n",
    "\n",
    "    def _evaluate(self, X, out, *args, **kwargs):\n",
    "        pop_size, _ = X.shape\n",
    "        F = np.zeros((pop_size, 1))\n",
    "\n",
    "        for k in range(pop_size):\n",
    "            p_vec = X[k, :]\n",
    "            total_reward = 0\n",
    "            penalty = 0\n",
    "            \n",
    "            for j, p in enumerate(p_vec):\n",
    "                # Individual view accuracy (not weighted)\n",
    "                acc = get_accuracy(p)\n",
    "                size = get_size(p)\n",
    "                \n",
    "                # Add penalties for constraint violations\n",
    "                if acc < min_accuracy:\n",
    "                    penalty += (min_accuracy - acc) ** 2 * 100\n",
    "                if size > max_model_sizes[j]:\n",
    "                    penalty += (size - max_model_sizes[j]) ** 2 * 10\n",
    "                \n",
    "                # Calculate reward for this view\n",
    "                r = get_reward(p, min_accuracy, max_model_sizes[j])\n",
    "                total_reward += r * i[j]\n",
    "            \n",
    "            # Minimize negative reward plus penalties\n",
    "            F[k, 0] = -(total_reward - penalty)\n",
    "\n",
    "        out[\"F\"] = F\n",
    "\n",
    "def is_feasible_corrected(p_vec):\n",
    "    \"\"\"Check if a pruning vector satisfies all constraints\"\"\"\n",
    "    for j, p in enumerate(p_vec):\n",
    "        acc = get_accuracy(p)  # Individual view accuracy\n",
    "        size = get_size(p)\n",
    "        \n",
    "        if acc < min_accuracy or size > max_model_sizes[j]:\n",
    "            return False, None, None\n",
    "    \n",
    "    # If feasible, return the actual values\n",
    "    accs = np.array([get_accuracy(p) for p in p_vec])\n",
    "    sizes = np.array([get_size(p) for p in p_vec])\n",
    "    return True, accs, sizes\n",
    "\n",
    "# Try multi-objective optimization first\n",
    "print(\"Running multi-objective optimization (NSGA-II)...\")\n",
    "problem = MultiViewProblem()\n",
    "algorithm = NSGA2(pop_size=100)\n",
    "\n",
    "res = minimize(\n",
    "    problem,\n",
    "    algorithm,\n",
    "    termination=('n_gen', 200),\n",
    "    seed=42,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "pareto_ps = res.X\n",
    "\n",
    "# Filter feasible solutions\n",
    "feasible = []\n",
    "for p_vec in pareto_ps:\n",
    "    valid, accs, sizes = is_feasible_corrected(p_vec)\n",
    "    if valid:\n",
    "        feasible.append((p_vec, accs, sizes))\n",
    "\n",
    "if feasible:\n",
    "    # Pick best feasible based on total weighted reward\n",
    "    best = None\n",
    "    best_reward = -np.inf\n",
    "    for p_vec, accs, sizes in feasible:\n",
    "        total_r = sum(get_reward(p, min_accuracy, max_model_sizes[j]) * i[j]\n",
    "                      for j, p in enumerate(p_vec))\n",
    "        if total_r > best_reward:\n",
    "            best_reward = total_r\n",
    "            best = (p_vec, accs, sizes, total_r)\n",
    "    \n",
    "    p_vec, accs, sizes, total_r = best\n",
    "    weighted_acc = np.sum(accs * i)\n",
    "    \n",
    "    print(\"\\n✦ FEASIBLE solution found with NSGA-II:\")\n",
    "    print(\"p values:         \", np.round(p_vec, 3))\n",
    "    print(\"view accuracies %:\", np.round(accs, 2))\n",
    "    print(\"weighted acc %:   \", np.round(weighted_acc, 2))\n",
    "    print(\"model sizes:      \", np.round(sizes, 1))\n",
    "    print(\"max sizes:        \", np.round(max_model_sizes, 1))\n",
    "    print(\"total reward:     \", np.round(total_r, 2))\n",
    "    \n",
    "    # Check constraints\n",
    "    acc_violations = sum(1 for acc in accs if acc < min_accuracy)\n",
    "    size_violations = sum(1 for j, size in enumerate(sizes) if size > max_model_sizes[j])\n",
    "    print(f\"Constraint violations: {acc_violations} accuracy, {size_violations} size\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo feasible solution found with NSGA-II. Trying single-objective optimization...\")\n",
    "    \n",
    "    # Try single objective optimization with penalties\n",
    "    single_problem = SingleObjectiveProblem()\n",
    "    single_algorithm = GA(pop_size=100)\n",
    "    \n",
    "    single_res = minimize(\n",
    "        single_problem,\n",
    "        single_algorithm,\n",
    "        termination=('n_gen', 300),\n",
    "        seed=42,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    best_p_vec = single_res.X\n",
    "    valid, accs, sizes = is_feasible_corrected(best_p_vec)\n",
    "    \n",
    "    if valid:\n",
    "        total_r = sum(get_reward(p, min_accuracy, max_model_sizes[j]) * i[j]\n",
    "                      for j, p in enumerate(best_p_vec))\n",
    "        weighted_acc = np.sum(accs * i)\n",
    "        \n",
    "        print(\"\\n✦ FEASIBLE solution found with single-objective GA:\")\n",
    "        print(\"p values:         \", np.round(best_p_vec, 3))\n",
    "        print(\"view accuracies %:\", np.round(accs, 2))\n",
    "        print(\"weighted acc %:   \", np.round(weighted_acc, 2))\n",
    "        print(\"model sizes:      \", np.round(sizes, 1))\n",
    "        print(\"max sizes:        \", np.round(max_model_sizes, 1))\n",
    "        print(\"total reward:     \", np.round(total_r, 2))\n",
    "    else:\n",
    "        # If still no feasible solution, relax constraints and show best compromise\n",
    "        accs = np.array([get_accuracy(p) for p in best_p_vec])\n",
    "        sizes = np.array([get_size(p) for p in best_p_vec])\n",
    "        weighted_acc = np.sum(accs * i)\n",
    "        \n",
    "        print(\"\\n✦ Best compromise solution (may violate some constraints):\")\n",
    "        print(\"p values:         \", np.round(best_p_vec, 3))\n",
    "        print(\"view accuracies %:\", np.round(accs, 2))\n",
    "        print(\"weighted acc %:   \", np.round(weighted_acc, 2))\n",
    "        print(\"model sizes:      \", np.round(sizes, 1))\n",
    "        print(\"max sizes:        \", np.round(max_model_sizes, 1))\n",
    "        \n",
    "        # Show constraint violations\n",
    "        acc_violations = [(j, acc, min_accuracy) for j, acc in enumerate(accs) if acc < min_accuracy]\n",
    "        size_violations = [(j, size, max_model_sizes[j]) for j, size in enumerate(sizes) if size > max_model_sizes[j]]\n",
    "        \n",
    "        if acc_violations:\n",
    "            print(\"\\nAccuracy violations:\")\n",
    "            for j, acc, min_acc in acc_violations:\n",
    "                print(f\"  View {j+1}: {acc:.1f}% < {min_acc}%\")\n",
    "        \n",
    "        if size_violations:\n",
    "            print(\"\\nSize violations:\")\n",
    "            for j, size, max_size in size_violations:\n",
    "                print(f\"  View {j+1}: {size:.1f} > {max_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55af9644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
