{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "823d24b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "# add path\n",
    "import sys\n",
    "sys.path.append('../../../MVCNN')\n",
    "from models import MVCNN\n",
    "from tools import ImgDataset\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94caf73e",
   "metadata": {},
   "source": [
    "# Calculating Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c174039e",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdbd4190",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2921957f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVCNN(\n",
       "  (net_1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU(inplace=True)\n",
       "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (net_2): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=33, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MVCNN.SVCNN('mvcnn')\n",
    "weights = torch.load('../../../MVCNN/MVCNN/model-mvcnn-00050.pth', map_location=device)\n",
    "model.load_state_dict(weights)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd234112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract CNN feature extractor from the model\n",
    "feature_extractor = model.net_1\n",
    "classifier = model.net_2\n",
    "\n",
    "feature_extractor.eval()\n",
    "classifier.eval()\n",
    "\n",
    "num_views = 12  # Number of views per model\n",
    "num_classes = 33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895be02c",
   "metadata": {},
   "source": [
    "## Accuracy Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c348a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global or at top of script\n",
    "view_order = [0, 1, 10, 11, 2, 3, 4, 5, 6, 7, 8, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d12fb86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(\n",
    "    model: torch.nn.Module,\n",
    "    loader: torch.utils.data.DataLoader,\n",
    "    num_classes: int,\n",
    "    device: torch.device,\n",
    "    single_view: bool = False,\n",
    "    view_idx: int = 0,\n",
    "    drop_view_label: Optional[int] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    - single_view: evaluate on only loader batch[:, view_idx]\n",
    "    - drop_view_label: the *label* of the view to drop (e.g. 5 → slice_idx 7)\n",
    "                       if None, no views are dropped.\n",
    "    \"\"\"\n",
    "\n",
    "    # map from semantic label → tensor‐slice index\n",
    "    if drop_view_label is not None:\n",
    "        assert drop_view_label in view_order, f\"{drop_view_label=} not in view_order\"\n",
    "        drop_slice = view_order.index(drop_view_label)\n",
    "    else:\n",
    "        drop_slice = None\n",
    "\n",
    "    model.eval()\n",
    "    total_correct = total_samples = 0\n",
    "    total_loss = 0.0\n",
    "    wrong_per_class = np.zeros(num_classes, dtype=int)\n",
    "    samples_per_class = np.zeros(num_classes, dtype=int)\n",
    "\n",
    "    pbar = tqdm(loader, desc=\"Validating\", unit=\"batch\", leave=False, dynamic_ncols=True)\n",
    "    for batch_i, data in enumerate(pbar):\n",
    "        labels = data[0].to(device)\n",
    "        views  = data[1].to(device)              # (N, 12, C, H, W)\n",
    "        N, V, C, H, W = views.shape\n",
    "\n",
    "        # drop the semantic view\n",
    "        if drop_slice is not None:\n",
    "            assert 0 <= drop_slice < V\n",
    "            keep = [i for i in range(V) if i != drop_slice]\n",
    "            views = views[:, keep]\n",
    "            V -= 1\n",
    "\n",
    "        # single-view path\n",
    "        if single_view:\n",
    "            assert 0 <= view_idx < V\n",
    "            x   = views[:, view_idx]           # (N, C, H, W)\n",
    "            tgt = labels                       # (N,)\n",
    "            with torch.no_grad():\n",
    "                out   = model(x)\n",
    "                loss  = F.cross_entropy(out, tgt).item()\n",
    "                preds = out.argmax(1)\n",
    "        # full-MVCNN path\n",
    "        else:\n",
    "            flat = views.reshape(-1, C, H, W)       # (N*V, C, H, W)\n",
    "            tgt  = labels.repeat_interleave(V, 0)   # (N*V,)\n",
    "            with torch.no_grad():\n",
    "                out   = model(flat)\n",
    "                loss  = F.cross_entropy(out, tgt).item()\n",
    "                preds = out.argmax(1).cpu().numpy() # flatten\n",
    "\n",
    "        total_loss += loss\n",
    "\n",
    "        if single_view:\n",
    "            correct_mask = (preds == tgt).cpu().numpy()\n",
    "            batch_correct = correct_mask.sum()\n",
    "            for i, ok in enumerate(correct_mask):\n",
    "                cls = tgt[i].item()\n",
    "                samples_per_class[cls] += 1\n",
    "                if not ok:\n",
    "                    wrong_per_class[cls] += 1\n",
    "        else:\n",
    "            preds = preds.reshape(N, V)    # (N, V)\n",
    "            voted = np.array([np.bincount(preds[i]).argmax() for i in range(N)])\n",
    "            gts   = labels.cpu().numpy()\n",
    "            batch_correct = (voted == gts).sum()\n",
    "            for i in range(N):\n",
    "                cls = gts[i]\n",
    "                samples_per_class[cls] += 1\n",
    "                if voted[i] != cls:\n",
    "                    wrong_per_class[cls] += 1\n",
    "\n",
    "        total_correct += batch_correct\n",
    "        total_samples += N\n",
    "\n",
    "        acc = batch_correct / N\n",
    "        avg_loss = total_loss / (batch_i + 1)\n",
    "        pbar.set_postfix(acc=f\"{acc:.4f}\", loss=f\"{avg_loss:.4f}\")\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    overall_acc = total_correct / total_samples\n",
    "    per_cls_acc = (samples_per_class - wrong_per_class) / np.maximum(samples_per_class, 1)\n",
    "    mean_cls_acc = per_cls_acc[samples_per_class > 0].mean()\n",
    "\n",
    "    drop_msg = f\", dropped view {drop_view_label}\" if drop_view_label is not None else \"\"\n",
    "    mode     = \"single-view\" if single_view else \"full-mvcnn\"\n",
    "    print(f\"\\n[{mode}{drop_msg}] Overall Acc: {overall_acc:.4f}   Mean Class Acc: {mean_cls_acc:.4f}\")\n",
    "\n",
    "    return overall_acc, mean_cls_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3874bb6",
   "metadata": {},
   "source": [
    "## Dataset Initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581563fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7952a225",
   "metadata": {},
   "source": [
    "### MVCNN Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f160f75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_mvcnn = ImgDataset.MultiviewImgDataset(\n",
    "    root_dir='../../../MVCNN/ModelNet40-12View/*/test',\n",
    "    scale_aug=False,\n",
    "    rot_aug=False,\n",
    "    test_mode=True,\n",
    "    num_models=0,\n",
    "    num_views=12,\n",
    ")\n",
    "test_loader_mvcnn = torch.utils.data.DataLoader(\n",
    "    test_dataset_mvcnn,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b8b003",
   "metadata": {},
   "source": [
    "### SVCNN Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b82b55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_svcnn = ImgDataset.SingleImgDataset(\n",
    "    root_dir='../../../MVCNN/ModelNet40-12View/*/test',\n",
    "    scale_aug=False,\n",
    "    rot_aug=False,\n",
    "    test_mode=True,\n",
    "    num_models=0,\n",
    ")\n",
    "test_loader_svcnn = torch.utils.data.DataLoader(\n",
    "    test_dataset_svcnn,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c69600",
   "metadata": {},
   "source": [
    "## Baseline Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64e2e9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mvalidate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_loader_mvcnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmps\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msingle_view\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_view_label\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mvalidate_model\u001b[39m\u001b[34m(model, loader, num_classes, device, single_view, view_idx, drop_view_label)\u001b[39m\n\u001b[32m     27\u001b[39m samples_per_class = np.zeros(num_classes, dtype=\u001b[38;5;28mint\u001b[39m)\n\u001b[32m     29\u001b[39m pbar = tqdm(loader, desc=\u001b[33m\"\u001b[39m\u001b[33mValidating\u001b[39m\u001b[33m\"\u001b[39m, unit=\u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m, leave=\u001b[38;5;28;01mFalse\u001b[39;00m, dynamic_ncols=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch_i, (labels, views) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pbar):\n\u001b[32m     31\u001b[39m     labels = labels.to(device)\n\u001b[32m     32\u001b[39m     views  = views.to(device)              \u001b[38;5;66;03m# (N, 12, C, H, W)\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "validate_model(model = model, loader=test_loader_mvcnn, num_classes=num_classes, device=torch.device('mps'), single_view=False, drop_view_label=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd1a6da",
   "metadata": {},
   "source": [
    "## Accuracy Of Each View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67bb6a75",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "validate_model() got an unexpected keyword argument 'drop_view'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m view_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m12\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mvalidate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmvcnn\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest_loader_mvcnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43msingle_view\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mview_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mview_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdrop_view\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: validate_model() got an unexpected keyword argument 'drop_view'"
     ]
    }
   ],
   "source": [
    "for view_idx in range(12):\n",
    "    validate_model(\n",
    "        model,\n",
    "        'mvcnn',\n",
    "        test_loader_mvcnn,\n",
    "        single_view=True,\n",
    "        view_idx=view_idx,\n",
    "        drop_view=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deee827",
   "metadata": {},
   "source": [
    "## Accuracy While Removing Views"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008acb7f",
   "metadata": {},
   "source": [
    "#### Use Multi View Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6b2dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 233/233 [01:39<00:00,  2.34batch/s, acc=1.0000, loss=1.3829]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Acc: 0.9188   Mean Class Acc: 0.8767 when view 0 is dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 233/233 [01:37<00:00,  2.38batch/s, acc=1.0000, loss=1.3977]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Acc: 0.9194   Mean Class Acc: 0.8806 when view 1 is dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 233/233 [01:39<00:00,  2.34batch/s, acc=1.0000, loss=1.3966]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Acc: 0.9194   Mean Class Acc: 0.8794 when view 2 is dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 233/233 [01:38<00:00,  2.37batch/s, acc=1.0000, loss=1.3726]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Acc: 0.9177   Mean Class Acc: 0.8773 when view 3 is dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 233/233 [01:37<00:00,  2.38batch/s, acc=1.0000, loss=1.3785]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Acc: 0.9167   Mean Class Acc: 0.8767 when view 4 is dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 233/233 [01:38<00:00,  2.38batch/s, acc=1.0000, loss=1.3832]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Acc: 0.9161   Mean Class Acc: 0.8764 when view 5 is dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 233/233 [01:37<00:00,  2.38batch/s, acc=1.0000, loss=1.3728]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Acc: 0.9177   Mean Class Acc: 0.8761 when view 6 is dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 233/233 [01:40<00:00,  2.31batch/s, acc=1.0000, loss=1.3880]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Acc: 0.9177   Mean Class Acc: 0.8785 when view 7 is dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 233/233 [01:41<00:00,  2.30batch/s, acc=1.0000, loss=1.3917]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Acc: 0.9172   Mean Class Acc: 0.8758 when view 8 is dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 233/233 [01:38<00:00,  2.36batch/s, acc=1.0000, loss=1.3689]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Acc: 0.9177   Mean Class Acc: 0.8773 when view 9 is dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 233/233 [01:37<00:00,  2.38batch/s, acc=1.0000, loss=1.3921]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Acc: 0.9188   Mean Class Acc: 0.8791 when view 10 is dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 233/233 [01:39<00:00,  2.34batch/s, acc=1.0000, loss=1.4021]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Acc: 0.9172   Mean Class Acc: 0.8770 when view 11 is dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for view_idx in range(12):\n",
    "    validate_model(\n",
    "        model,\n",
    "        'mvcnn',\n",
    "        test_loader_mvcnn,\n",
    "        single_view=False,\n",
    "        drop_view=view_idx\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6da6c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
