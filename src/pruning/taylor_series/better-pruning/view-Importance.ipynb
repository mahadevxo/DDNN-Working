{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "823d24b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import MVCNN\n",
    "from tools import ImgDataset\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94caf73e",
   "metadata": {},
   "source": [
    "# Calculating Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c174039e",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdbd4190",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2921957f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVCNN(\n",
       "  (net_1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU(inplace=True)\n",
       "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (net_2): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=33, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MVCNN.SVCNN('svcnn')\n",
    "weights = torch.load('../../../MVCNN/MVCNN/model-00050.pth', map_location=device)\n",
    "model.load_state_dict(weights)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd234112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract CNN feature extractor from the model\n",
    "feature_extractor = model.net_1\n",
    "classifier = model.net_2\n",
    "\n",
    "feature_extractor.eval()\n",
    "classifier.eval()\n",
    "\n",
    "num_views = 12  # Number of views per model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c69600",
   "metadata": {},
   "source": [
    "## Baseline Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53174150",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ImgDataset.MultiviewImgDataset(\n",
    "    root_dir='../../../MVCNN/ModelNet40-12View/*/test',\n",
    "    scale_aug=False,\n",
    "    rot_aug=False,\n",
    "    test_mode=True,\n",
    "    num_models=100,\n",
    "    num_views=12,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12fb86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_correct = 0\n",
    "baseline_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for labels, views, paths in tqdm(test_loader, desc=\"Baseline Eval\"):\n",
    "        # Expected input: views shape = [batch_size, num_views, C, H, W]\n",
    "        # But you're getting: views shape = [num_views, batch_size, C, H, W]\n",
    "        \n",
    "        # First, transpose to match expected format\n",
    "        if views.dim() == 5:\n",
    "            # If views is [V, N, C, H, W], transpose to [N, V, C, H, W]\n",
    "            if views.shape[0] == 12:  # assuming 12 views\n",
    "                views = views.transpose(0, 1)  # Now [N, V, C, H, W]\n",
    "        \n",
    "        N, V, C, H, W = views.shape\n",
    "        \n",
    "        # Reshape exactly like the original validation code\n",
    "        in_data = views.view(-1, C, H, W).to(device)  # [N*V, C, H, W]\n",
    "        target = labels.to(device).repeat_interleave(V)  # [N*V]\n",
    "        \n",
    "        # Process through the model (like original validation)\n",
    "        # If using separate feature extractor and classifier\n",
    "        features = feature_extractor(in_data)  # [N*V, feature_dim, H', W']\n",
    "        \n",
    "        # Pool features\n",
    "        pooled_features = torch.nn.functional.adaptive_avg_pool2d(features, (7, 7))\n",
    "        pooled_features = pooled_features.view(N*V, -1)  # [N*V, feature_dim]\n",
    "        \n",
    "        # Get predictions for all views\n",
    "        out_data = classifier(pooled_features)  # [N*V, num_classes]\n",
    "            \n",
    "            # If using a single end-to-end model\n",
    "        out_data = model(in_data)  # Replace 'model' with your actual model\n",
    "        \n",
    "        pred = torch.max(out_data, 1)[1]  # [N*V]\n",
    "        \n",
    "        # Now do majority voting like the original validation code\n",
    "        for i in range(N):\n",
    "            obj_preds = pred[i*V:(i+1)*V]  # Predictions for all views of object i\n",
    "            obj_target = target[i*V]  # True label for object i\n",
    "            \n",
    "            # Majority voting across views\n",
    "            obj_pred = torch.mode(obj_preds.cpu())[0].to(device)\n",
    "            \n",
    "            if obj_pred == obj_target:\n",
    "                baseline_correct += 1\n",
    "            baseline_total += 1\n",
    "\n",
    "print(f\"Baseline Accuracy: {baseline_correct/baseline_total:.4f}\")\n",
    "print(f\"Total objects processed: {baseline_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd1a6da",
   "metadata": {},
   "source": [
    "## Accuracy Of Each View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b448318b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Single View Analysis: 100%|██████████| 22320/22320 [02:52<00:00, 129.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy using only one view:\n",
      "View 0 only: 0.7005\n",
      "View 1 only: 0.7522\n",
      "View 10 only: 0.7699\n",
      "View 11 only: 0.7435\n",
      "View 2 only: 0.7522\n",
      "View 3 only: 0.7312\n",
      "View 4 only: 0.7349\n",
      "View 5 only: 0.6984\n",
      "View 6 only: 0.6285\n",
      "View 7 only: 0.7118\n",
      "View 8 only: 0.7285\n",
      "View 9 only: 0.7312\n",
      "\n",
      "Views ranked by accuracy:\n",
      "View 10: 0.7699\n",
      "View 1: 0.7522\n",
      "View 2: 0.7522\n",
      "View 11: 0.7435\n",
      "View 4: 0.7349\n",
      "View 3: 0.7312\n",
      "View 9: 0.7312\n",
      "View 8: 0.7285\n",
      "View 7: 0.7118\n",
      "View 0: 0.7005\n",
      "View 5: 0.6984\n",
      "View 6: 0.6285\n",
      "\n",
      "Best single view: View 10 (0.7699)\n",
      "Worst single view: View 6 (0.6285)\n",
      "Average single view accuracy: 0.7236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_dataset = ImgDataset.SingleImgDataset(\n",
    "    root_dir='../../../MVCNN/ModelNet40-12View/*/test',\n",
    "    scale_aug=False,\n",
    "    rot_aug=False,\n",
    "    test_mode=True,\n",
    "    num_models=100,\n",
    "    num_views=12,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\"---------------------------------------------------------------------------------------------------------------------\"\n",
    "\n",
    "# More efficient version - processes all single views at once\n",
    "single_view_results = {i: {'correct': 0, 'total': 0} for i in range(num_views)}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for labels, views, paths in tqdm(test_loader, desc=\"Single View Analysis\"):\n",
    "        # SingleImgDataset should return [batch_size, C, H, W]\n",
    "        if views.dim() == 4:\n",
    "            N, C, H, W = views.shape\n",
    "            view_data = views.to(device)\n",
    "            target = labels.to(device)\n",
    "            \n",
    "            # Get predictions using the full model\n",
    "            out_data = model(view_data)\n",
    "            pred = torch.max(out_data, 1)[1]\n",
    "            \n",
    "            # Extract view index from paths and accumulate results\n",
    "            for i in range(N):\n",
    "                path = paths[i]\n",
    "                # Extract view index from path (assuming format like \"object_shaded_X.png\")\n",
    "                view_idx = int(path.split('_')[-1].split('.')[0])\n",
    "                \n",
    "                correct = (pred[i] == target[i]).item()\n",
    "                single_view_results[view_idx]['correct'] += correct\n",
    "                single_view_results[view_idx]['total'] += 1\n",
    "        else:\n",
    "            print(f\"Unexpected tensor shape: {views.shape}\")\n",
    "            break\n",
    "\n",
    "# Print results\n",
    "view_order = [0, 1, 10, 11, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "print(\"\\nAccuracy using only one view:\")\n",
    "for view_idx in view_order:\n",
    "    if single_view_results[view_idx]['total'] > 0:\n",
    "        accuracy = single_view_results[view_idx]['correct'] / single_view_results[view_idx]['total']\n",
    "        print(f\"View {view_idx} only: {accuracy:.4f}\")\n",
    "\n",
    "# Summary statistics\n",
    "accuracies = [(view_idx, single_view_results[view_idx]['correct'] / single_view_results[view_idx]['total']) \n",
    "              for view_idx in range(num_views) if single_view_results[view_idx]['total'] > 0]\n",
    "accuracies.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nViews ranked by accuracy:\")\n",
    "for view_idx, acc in accuracies:\n",
    "    print(f\"View {view_idx}: {acc:.4f}\")\n",
    "\n",
    "print(f\"\\nBest single view: View {accuracies[0][0]} ({accuracies[0][1]:.4f})\")\n",
    "print(f\"Worst single view: View {accuracies[-1][0]} ({accuracies[-1][1]:.4f})\")\n",
    "print(f\"Average single view accuracy: {sum(acc for _, acc in accuracies) / len(accuracies):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deee827",
   "metadata": {},
   "source": [
    "## Accuracy While Removing Views"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008acb7f",
   "metadata": {},
   "source": [
    "#### Use Multi View Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f426233",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Drop View Analysis: 100%|██████████| 233/233 [16:04<00:00,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy when dropping each view:\n",
      "Drop view 0: 0.8978\n",
      "Drop view 1: 0.8973\n",
      "Drop view 10: 0.9000\n",
      "Drop view 11: 0.8978\n",
      "Drop view 2: 0.9005\n",
      "Drop view 3: 0.9000\n",
      "Drop view 4: 0.8995\n",
      "Drop view 5: 0.8989\n",
      "Drop view 6: 0.8995\n",
      "Drop view 7: 0.9016\n",
      "Drop view 8: 0.8984\n",
      "Drop view 9: 0.8989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_dataset = ImgDataset.MultiviewImgDataset(\n",
    "    root_dir='../../../MVCNN/ModelNet40-12View/*/test',\n",
    "    scale_aug=False,\n",
    "    rot_aug=False,\n",
    "    test_mode=True,\n",
    "    num_models=100,\n",
    "    num_views=12,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\"---------------------------------------------------------------------------------------------------------------------\"\n",
    "\n",
    "drop_results = {i: {'correct': 0, 'total': 0} for i in range(num_views)}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for labels, views, paths in tqdm(test_loader, desc=\"Drop View Analysis\", dynamic_ncols=True):\n",
    "        if views.dim() == 5 and views.shape[0] == 12:\n",
    "            views = views.transpose(0, 1)  # Now [N, V, C, H, W]\n",
    "        \n",
    "        N, V, C, H, W = views.shape\n",
    "        \n",
    "        # For each object, test dropping each view\n",
    "        for i in range(N):\n",
    "            obj_views = views[i]  # [V, C, H, W] - all 12 views for object i\n",
    "            obj_label = labels[i].to(device)\n",
    "            \n",
    "            # Test dropping each view\n",
    "            for drop_view in range(V):\n",
    "                # Create tensor with all views except the dropped one\n",
    "                remaining_views = torch.cat([\n",
    "                    obj_views[:drop_view], \n",
    "                    obj_views[drop_view+1:]\n",
    "                ], dim=0).to(device)  # [V-1, C, H, W]\n",
    "                \n",
    "                # Get predictions for remaining views only\n",
    "                features = feature_extractor(remaining_views)\n",
    "                pooled_features = torch.nn.functional.adaptive_avg_pool2d(features, (7, 7))\n",
    "                pooled_features = pooled_features.view(V-1, -1)\n",
    "                out_data = classifier(pooled_features)\n",
    "                preds = torch.max(out_data, 1)[1]  # [V-1]\n",
    "                \n",
    "                # Majority voting on remaining predictions\n",
    "                obj_pred = torch.mode(preds.cpu())[0].to(device)\n",
    "                \n",
    "                if obj_pred == obj_label:\n",
    "                    drop_results[drop_view]['correct'] += 1\n",
    "                drop_results[drop_view]['total'] += 1\n",
    "\n",
    "# Print results\n",
    "view_order = [0, 1, 10, 11, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "print(\"\\nAccuracy when dropping each view:\")\n",
    "for view_idx in view_order:\n",
    "    accuracy = drop_results[view_idx]['correct'] / drop_results[view_idx]['total']\n",
    "    print(f\"Drop view {view_idx}: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6b2dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
