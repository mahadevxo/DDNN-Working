{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "823d24b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import MVCNN\n",
    "from tools import ImgDataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94caf73e",
   "metadata": {},
   "source": [
    "# Calculating Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c174039e",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdbd4190",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2921957f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVCNN(\n",
       "  (net_1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU(inplace=True)\n",
       "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (net_2): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=33, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MVCNN.SVCNN('svcnn')\n",
    "weights = torch.load('../../../MVCNN/MVCNN/model-00050.pth', map_location=device)\n",
    "model.load_state_dict(weights)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32815c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ImgDataset.MultiviewImgDataset(\n",
    "    root_dir='../../../MVCNN/ModelNet40-12View/*/test',\n",
    "    scale_aug=False,\n",
    "    rot_aug=False,\n",
    "    test_mode=True,\n",
    "    num_models=1000,\n",
    "    num_views=12,\n",
    "    shuffle=False\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd234112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract CNN feature extractor from the model\n",
    "feature_extractor = model.net_1\n",
    "classifier = model.net_2\n",
    "\n",
    "feature_extractor.eval()\n",
    "classifier.eval()\n",
    "\n",
    "num_views = 12  # Number of views per model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943d291a",
   "metadata": {},
   "source": [
    "views are as\n",
    "\n",
    "0, 1, 10, 11, 2, 3, 4, 5, 6, 7, 8, 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd1a6da",
   "metadata": {},
   "source": [
    "## Accuracy Of Each View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee5599ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking = {\n",
    "    'view 0': [], \n",
    "    'view 1': [],\n",
    "    'view 2': [],\n",
    "    'view 3': [],\n",
    "    'view 4': [],\n",
    "    'view 5': [],\n",
    "    'view 6': [],\n",
    "    'view 7': [],\n",
    "    'view 8': [],\n",
    "    'view 9': [],\n",
    "    'view 10': [],\n",
    "    'view 11': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "360f8bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79adf468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for view 0:  0.8338709677419355\n",
      "Validation loss:  0.8813561098326828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for view 1:  0.8575268817204301\n",
      "Validation loss:  0.9082475790514812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for view 10:  0.853763440860215\n",
      "Validation loss:  0.8412012453337213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for view 11:  0.8376344086021505\n",
      "Validation loss:  0.8129057454942058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for view 2:  0.8521505376344086\n",
      "Validation loss:  0.7703247849596861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for view 3:  0.8543010752688172\n",
      "Validation loss:  0.7904298889504381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for view 4:  0.8693548387096774\n",
      "Validation loss:  0.7775200685571102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for view 5:  0.8424731182795699\n",
      "Validation loss:  0.7697400303302722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for view 6:  0.85\n",
      "Validation loss:  0.7882564354094462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for view 7:  0.8225806451612904\n",
      "Validation loss:  0.9045681064739782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for view 8:  0.8016129032258065\n",
      "Validation loss:  1.11109438548692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for view 9:  0.8408602150537634\n",
      "Validation loss:  0.8763477378425916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "for view in [0, 1, 10, 11, 2, 3, 4, 5, 6, 7, 8, 9]:\n",
    "    model.eval()\n",
    "    samples_class = torch.zeros(33)\n",
    "    wrong_class = torch.zeros(33)\n",
    "    all_points = 0\n",
    "    all_correct_points = 0\n",
    "    all_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(test_loader, desc=\"Validating\", leave=False, ncols=80)\n",
    "        for _, data in enumerate(pbar, 0):\n",
    "            data[1] = data[1][:, view, :, :, :].to(device)  # [N, C, H, W]\n",
    "            N, C, H, W = data[1].size()\n",
    "            in_data = data[1]\n",
    "            target = data[0].to(device)\n",
    "\n",
    "            out_data = model(in_data)\n",
    "            pred = torch.max(out_data, 1)[1]\n",
    "            all_loss += torch.nn.functional.cross_entropy(out_data, target).item()\n",
    "            results = pred == target\n",
    "\n",
    "            for i in range(N):\n",
    "                obj_pred = pred[i]\n",
    "                obj_target = target[i]\n",
    "                if obj_target != obj_pred:\n",
    "                    wrong_class[obj_target.item()] += 1\n",
    "                samples_class[obj_target.item()] += 1\n",
    "\n",
    "            correct = torch.sum(results.long())\n",
    "            all_correct_points += correct.item()\n",
    "            all_points += results.size(0)\n",
    "\n",
    "    print(f\"Validation accuracy for view {view}: \", all_correct_points / all_points)\n",
    "    print(\"Validation loss: \", all_loss / len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aea10118",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1860/1860 [02:26<00:00, 12.72it/s]\n"
     ]
    }
   ],
   "source": [
    "prev=torch.tensor([]).to(device)\n",
    "for batch_idx, (target, data, data_path) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "    data = data.to(device)\n",
    "    target = target.to(device)\n",
    "    for i, view in enumerate([0, 1, 10, 11, 2, 3, 4, 5, 6, 7, 8, 9]):\n",
    "        view_data = data[:, view, :, :, :].squeeze(0)\n",
    "        view_data = view_data.to(device)\n",
    "        \n",
    "        if torch.equal(view_data, prev):\n",
    "            break\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = feature_extractor(view_data.unsqueeze(0))\n",
    "            pooled = torch.max(features, dim=0)[0].unsqueeze(0)\n",
    "            pooled = pooled.view(pooled.size(0), -1)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = classifier(pooled)\n",
    "            \n",
    "            _, pred = torch.max(output, 1)\n",
    "            out = (pred==target).float().item()\n",
    "            ranking[f'view {view}'].append(out)\n",
    "        \n",
    "        prev = view_data\n",
    "        \n",
    "        with open (\"outputs\", 'a') as f:\n",
    "            f.write(f\"{data_path[i]} view {view}, {out}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91b1e4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking = {view: sum(ranking[view]) / len(ranking[view]) if ranking[view] else 0.0 for view in ranking}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3637e836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'view 0': 0.8338709677419355,\n",
       " 'view 1': 0.8575268817204301,\n",
       " 'view 2': 0.8521505376344086,\n",
       " 'view 3': 0.8543010752688172,\n",
       " 'view 4': 0.8693548387096774,\n",
       " 'view 5': 0.8424731182795699,\n",
       " 'view 6': 0.85,\n",
       " 'view 7': 0.8225806451612904,\n",
       " 'view 8': 0.8016129032258065,\n",
       " 'view 9': 0.8408602150537634,\n",
       " 'view 10': 0.853763440860215,\n",
       " 'view 11': 0.8376344086021505}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91698e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_norm(arr):\n",
    "    max_val = max(arr)\n",
    "    min_val = min(arr)\n",
    "    return [(x-min_val) / (max_val - min_val) for x in arr]\n",
    "\n",
    "def l1_norm(arr):\n",
    "    norm = sum(abs(x) for x in arr)\n",
    "    return [(x)*10 / norm for x in arr] if norm != 0 else arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7266c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4761904761904757,\n",
       " 0.8253968253968252,\n",
       " 0.7460317460317459,\n",
       " 0.7777777777777767,\n",
       " 1.0,\n",
       " 0.6031746031746036,\n",
       " 0.7142857142857136,\n",
       " 0.30952380952380987,\n",
       " 0.0,\n",
       " 0.5793650793650784,\n",
       " 0.7698412698412694,\n",
       " 0.5317460317460315]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_norm(list(ranking.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87f23863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.824298469387755,\n",
       " 0.8476828231292517,\n",
       " 0.8423681972789114,\n",
       " 0.8444940476190476,\n",
       " 0.8593749999999999,\n",
       " 0.8328018707482994,\n",
       " 0.8402423469387754,\n",
       " 0.8131377551020408,\n",
       " 0.7924107142857142,\n",
       " 0.8312074829931971,\n",
       " 0.8439625850340134,\n",
       " 0.8280187074829932]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_norm(list(ranking.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deee827",
   "metadata": {},
   "source": [
    "## Accuracy While Removing Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f672cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ImgDataset.MultiviewImgDataset(\n",
    "    root_dir='../../../MVCNN/ModelNet40-12View/*/test',\n",
    "    scale_aug=False,\n",
    "    rot_aug=False,\n",
    "    test_mode=True,\n",
    "    # num_models=100,\n",
    "    num_views=12,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3b095b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../../../MVCNN/ModelNet40-12View/*/train'\n",
    "test_path = '../../../MVCNN/ModelNet40-12View/*/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aec6ed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_view_importance = {\n",
    "    0: {\n",
    "        'samples': torch.zeros(33),\n",
    "        'wrong': torch.zeros(33),\n",
    "        'all_points': 0,\n",
    "        'all_correct': 0,\n",
    "    },\n",
    "    1: {\n",
    "        'samples': torch.zeros(33),\n",
    "        'wrong': torch.zeros(33),\n",
    "        'all_points': 0,\n",
    "        'all_correct': 0,\n",
    "    },\n",
    "    2: {\n",
    "        'samples': torch.zeros(33),\n",
    "        'wrong': torch.zeros(33),\n",
    "        'all_points': 0,\n",
    "        'all_correct': 0,\n",
    "    },\n",
    "    3: {\n",
    "        'samples': torch.zeros(33),\n",
    "        'wrong': torch.zeros(33),\n",
    "        'all_points': 0,\n",
    "        'all_correct': 0,\n",
    "    },\n",
    "    4: {\n",
    "        'samples': torch.zeros(33),\n",
    "        'wrong': torch.zeros(33),\n",
    "        'all_points': 0,\n",
    "        'all_correct': 0,\n",
    "    },\n",
    "    5: {\n",
    "        'samples': torch.zeros(33),\n",
    "        'wrong': torch.zeros(33),\n",
    "        'all_points': 0,\n",
    "        'all_correct': 0,\n",
    "    },\n",
    "    6: {\n",
    "        'samples': torch.zeros(33),\n",
    "        'wrong': torch.zeros(33),\n",
    "        'all_points': 0,\n",
    "        'all_correct': 0,\n",
    "    },\n",
    "    7: {\n",
    "        'samples': torch.zeros(33),\n",
    "        'wrong': torch.zeros(33),\n",
    "        'all_points': 0,\n",
    "        'all_correct': 0,\n",
    "    },\n",
    "    8: {\n",
    "        'samples': torch.zeros(33),\n",
    "        'wrong': torch.zeros(33),\n",
    "        'all_points': 0,\n",
    "        'all_correct': 0,\n",
    "    },\n",
    "    9: {\n",
    "        'samples': torch.zeros(33),\n",
    "        'wrong': torch.zeros(33),\n",
    "        'all_points': 0,\n",
    "        'all_correct': 0,\n",
    "    },\n",
    "    10: {\n",
    "        'samples': torch.zeros(33),\n",
    "        'wrong': torch.zeros(33),\n",
    "        'all_points': 0,\n",
    "        'all_correct': 0,\n",
    "    },\n",
    "    11: {\n",
    "        'samples': torch.zeros(33),\n",
    "        'wrong': torch.zeros(33),\n",
    "        'all_points': 0,\n",
    "        'all_correct': 0,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c574f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|                                      | 0/1860 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "samples_class_baseline = torch.zeros(33)\n",
    "wrong_class_baseline = torch.zeros(33)\n",
    "all_points = 0\n",
    "all_correct_points_baseline = 0\n",
    "all_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    pbar = tqdm(test_loader, desc=\"Validating\", leave=False, ncols=80)\n",
    "    for batch_idx, data in enumerate(pbar, 0):\n",
    "        N, V, C, H, W = data[1].size()\n",
    "        \n",
    "        in_data = data[1].view(-1, C, H, W).to(device)\n",
    "        target = data[0].to(device).repeat_interleave(V)\n",
    "        \n",
    "        out_data = model(in_data)\n",
    "        pred = torch.max(out_data, 1)[1]\n",
    "        all_loss += torch.nn.functional.cross_entropy(out_data, target).item()\n",
    "        results = pred == target\n",
    "        \n",
    "        for i in range(N):\n",
    "            obj_preds = pred[i*V:(i+1)*V]\n",
    "            obj_pred = torch.mode(obj_preds.to('cpu'))[0].to(device)\n",
    "            obj_target = target[i*V]\n",
    "            \n",
    "            if obj_target != obj_pred:\n",
    "                wrong_class_baseline[obj_target.item()] += 1\n",
    "            samples_class_baseline[obj_target.item()] += 1\n",
    "        correct = torch.sum(results.long())\n",
    "        all_correct_points_baseline += correct.item()\n",
    "        all_points += results.size()[0]\n",
    "        \n",
    "        for view in tqdm([0, 1, 10, 11, 2, 3, 4, 5, 6, 7, 8, 9], desc=f\"View removal batch {batch_idx}\", leave=False):\n",
    "            in_data = data[1][:, [i for i in range(V) if i != view], :, :, :]\n",
    "            in_data = in_data.contiguous().view(-1, C, H, W).to(device)\n",
    "            target_view_removed = data[0].to(device).repeat_interleave(V - 1)\n",
    "            out_data = model(in_data)\n",
    "            pred = torch.max(out_data, 1)[1]\n",
    "            results = pred == target_view_removed\n",
    "            \n",
    "            for i in range(N):\n",
    "                obj_preds = pred[i*(V-1):(i+1)*(V-1)]\n",
    "                obj_pred = torch.mode(obj_preds.to('cpu'))[0].to(device)\n",
    "                obj_target = target[i*V]\n",
    "                if obj_preds.numel() == 0:\n",
    "                    continue\n",
    "                if obj_target != obj_pred:\n",
    "                    per_view_importance[view]['wrong'][obj_target.item()] += 1\n",
    "                # print(obj_target.item(), obj_pred.item(), view)\n",
    "                per_view_importance[view]['samples'][obj_target.item()] += 1\n",
    "            correct = torch.sum(results.long())\n",
    "            per_view_importance[view]['all_correct']+=correct.item()\n",
    "            per_view_importance[view]['all_points']+=results.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cd9c61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'samples': tensor([100., 100.,  20., 100., 100.,  20., 100., 100.,  20.,  20.,  20.,  20.,\n",
       "           20., 100., 100.,  20.,  20.,  20., 100.,  20., 100., 100.,  20., 100.,\n",
       "           20.,  20.,  20.,  20., 100., 100., 100.,  20.,  20.]),\n",
       "  'wrong': tensor([ 0.,  0.,  8.,  3.,  4.,  1.,  0.,  3.,  4.,  5.,  4.,  2., 16.,  5.,\n",
       "           8.,  1.,  3.,  0.,  6.,  2., 11., 11., 11., 13.,  6.,  6.,  6.,  3.,\n",
       "           2., 19., 17.,  3.,  7.]),\n",
       "  'all_points': 20460,\n",
       "  'all_correct': 17265},\n",
       " 1: {'samples': tensor([100., 100.,  20., 100., 100.,  20., 100., 100.,  20.,  20.,  20.,  20.,\n",
       "           20., 100., 100.,  20.,  20.,  20., 100.,  20., 100., 100.,  20., 100.,\n",
       "           20.,  20.,  20.,  20., 100., 100., 100.,  20.,  20.]),\n",
       "  'wrong': tensor([ 0.,  0.,  7.,  3.,  5.,  1.,  0.,  3.,  4.,  4.,  4.,  2., 16.,  4.,\n",
       "           7.,  1.,  3.,  0.,  7.,  2., 10., 12., 12., 13.,  7.,  6.,  6.,  3.,\n",
       "           1., 20., 19.,  3.,  6.]),\n",
       "  'all_points': 20460,\n",
       "  'all_correct': 17221},\n",
       " 2: {'samples': tensor([100., 100.,  20., 100., 100.,  20., 100., 100.,  20.,  20.,  20.,  20.,\n",
       "           20., 100., 100.,  20.,  20.,  20., 100.,  20., 100., 100.,  20., 100.,\n",
       "           20.,  20.,  20.,  20., 100., 100., 100.,  20.,  20.]),\n",
       "  'wrong': tensor([ 0.,  1.,  8.,  3.,  3.,  1.,  0.,  3.,  4.,  5.,  4.,  2., 17.,  5.,\n",
       "           8.,  1.,  5.,  0.,  6.,  2.,  9., 11.,  9., 13.,  7.,  6.,  5.,  3.,\n",
       "           2., 17., 17.,  3.,  5.]),\n",
       "  'all_points': 20460,\n",
       "  'all_correct': 17231},\n",
       " 3: {'samples': tensor([100., 100.,  20., 100., 100.,  20., 100., 100.,  20.,  20.,  20.,  20.,\n",
       "           20., 100., 100.,  20.,  20.,  20., 100.,  20., 100., 100.,  20., 100.,\n",
       "           20.,  20.,  20.,  20., 100., 100., 100.,  20.,  20.]),\n",
       "  'wrong': tensor([ 0.,  0.,  7.,  3.,  4.,  1.,  0.,  4.,  4.,  5.,  4.,  2., 16.,  4.,\n",
       "           7.,  1.,  4.,  0.,  6.,  1.,  9., 13., 11., 12.,  7.,  6.,  6.,  2.,\n",
       "           2., 18., 18.,  3.,  6.]),\n",
       "  'all_points': 20460,\n",
       "  'all_correct': 17227},\n",
       " 4: {'samples': tensor([100., 100.,  20., 100., 100.,  20., 100., 100.,  20.,  20.,  20.,  20.,\n",
       "           20., 100., 100.,  20.,  20.,  20., 100.,  20., 100., 100.,  20., 100.,\n",
       "           20.,  20.,  20.,  20., 100., 100., 100.,  20.,  20.]),\n",
       "  'wrong': tensor([ 0.,  0.,  7.,  3.,  4.,  1.,  0.,  3.,  4.,  6.,  4.,  2., 17.,  4.,\n",
       "           8.,  1.,  3.,  0.,  7.,  1., 11., 11., 11., 14.,  7.,  5.,  5.,  3.,\n",
       "           1., 18., 17.,  3.,  6.]),\n",
       "  'all_points': 20460,\n",
       "  'all_correct': 17199},\n",
       " 5: {'samples': tensor([100., 100.,  20., 100., 100.,  20., 100., 100.,  20.,  20.,  20.,  20.,\n",
       "           20., 100., 100.,  20.,  20.,  20., 100.,  20., 100., 100.,  20., 100.,\n",
       "           20.,  20.,  20.,  20., 100., 100., 100.,  20.,  20.]),\n",
       "  'wrong': tensor([ 0.,  0.,  7.,  3.,  4.,  1.,  0.,  4.,  4.,  6.,  4.,  2., 17.,  5.,\n",
       "           8.,  1.,  4.,  0.,  6.,  1., 11., 11., 11., 13.,  6.,  5.,  5.,  2.,\n",
       "           1., 18., 19.,  3.,  6.]),\n",
       "  'all_points': 20460,\n",
       "  'all_correct': 17249},\n",
       " 6: {'samples': tensor([100., 100.,  20., 100., 100.,  20., 100., 100.,  20.,  20.,  20.,  20.,\n",
       "           20., 100., 100.,  20.,  20.,  20., 100.,  20., 100., 100.,  20., 100.,\n",
       "           20.,  20.,  20.,  20., 100., 100., 100.,  20.,  20.]),\n",
       "  'wrong': tensor([ 0.,  1.,  7.,  3.,  4.,  1.,  0.,  3.,  4.,  5.,  4.,  2., 17.,  5.,\n",
       "           8.,  1.,  4.,  0.,  7.,  1., 11., 12., 10., 12.,  7.,  6.,  5.,  2.,\n",
       "           1., 17., 18.,  3.,  6.]),\n",
       "  'all_points': 20460,\n",
       "  'all_correct': 17235},\n",
       " 7: {'samples': tensor([100., 100.,  20., 100., 100.,  20., 100., 100.,  20.,  20.,  20.,  20.,\n",
       "           20., 100., 100.,  20.,  20.,  20., 100.,  20., 100., 100.,  20., 100.,\n",
       "           20.,  20.,  20.,  20., 100., 100., 100.,  20.,  20.]),\n",
       "  'wrong': tensor([ 0.,  1.,  7.,  3.,  4.,  1.,  0.,  3.,  4.,  5.,  4.,  2., 16.,  4.,\n",
       "           7.,  1.,  3.,  0.,  7.,  1.,  9., 11., 10., 12.,  7.,  5.,  6.,  2.,\n",
       "           1., 19., 19.,  3.,  6.]),\n",
       "  'all_points': 20460,\n",
       "  'all_correct': 17286},\n",
       " 8: {'samples': tensor([100., 100.,  20., 100., 100.,  20., 100., 100.,  20.,  20.,  20.,  20.,\n",
       "           20., 100., 100.,  20.,  20.,  20., 100.,  20., 100., 100.,  20., 100.,\n",
       "           20.,  20.,  20.,  20., 100., 100., 100.,  20.,  20.]),\n",
       "  'wrong': tensor([ 0.,  0.,  7.,  3.,  4.,  1.,  0.,  3.,  4.,  5.,  4.,  2., 16.,  5.,\n",
       "           7.,  1.,  4.,  0.,  7.,  2.,  9., 12., 12., 13.,  6.,  5.,  6.,  2.,\n",
       "           1., 19., 20.,  3.,  6.]),\n",
       "  'all_points': 20460,\n",
       "  'all_correct': 17325},\n",
       " 9: {'samples': tensor([100., 100.,  20., 100., 100.,  20., 100., 100.,  20.,  20.,  20.,  20.,\n",
       "           20., 100., 100.,  20.,  20.,  20., 100.,  20., 100., 100.,  20., 100.,\n",
       "           20.,  20.,  20.,  20., 100., 100., 100.,  20.,  20.]),\n",
       "  'wrong': tensor([ 0.,  0.,  8.,  3.,  4.,  1.,  0.,  3.,  4.,  4.,  4.,  2., 16.,  4.,\n",
       "           8.,  1.,  4.,  0.,  7.,  2.,  9., 13., 12., 12.,  8.,  6.,  6.,  2.,\n",
       "           1., 18., 18.,  3.,  5.]),\n",
       "  'all_points': 20460,\n",
       "  'all_correct': 17252},\n",
       " 10: {'samples': tensor([100., 100.,  20., 100., 100.,  20., 100., 100.,  20.,  20.,  20.,  20.,\n",
       "           20., 100., 100.,  20.,  20.,  20., 100.,  20., 100., 100.,  20., 100.,\n",
       "           20.,  20.,  20.,  20., 100., 100., 100.,  20.,  20.]),\n",
       "  'wrong': tensor([ 0.,  1.,  7.,  3.,  4.,  1.,  0.,  3.,  4.,  5.,  4.,  2., 17.,  5.,\n",
       "           8.,  1.,  4.,  0.,  6.,  2., 10., 13., 10., 11.,  7.,  6.,  5.,  3.,\n",
       "           1., 18., 18.,  3.,  4.]),\n",
       "  'all_points': 20460,\n",
       "  'all_correct': 17228},\n",
       " 11: {'samples': tensor([100., 100.,  20., 100., 100.,  20., 100., 100.,  20.,  20.,  20.,  20.,\n",
       "           20., 100., 100.,  20.,  20.,  20., 100.,  20., 100., 100.,  20., 100.,\n",
       "           20.,  20.,  20.,  20., 100., 100., 100.,  20.,  20.]),\n",
       "  'wrong': tensor([ 0.,  1.,  7.,  3.,  3.,  1.,  0.,  4.,  4.,  5.,  4.,  2., 17.,  5.,\n",
       "           8.,  1.,  3.,  0.,  7.,  2., 11., 12., 11., 13.,  7.,  6.,  5.,  2.,\n",
       "           1., 18., 19.,  3.,  5.]),\n",
       "  'all_points': 20460,\n",
       "  'all_correct': 17258}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_view_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b033dbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Baseline Evaluation: 100%|██████████████████| 1860/1860 [02:09<00:00, 14.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline object-level accuracy: 0.9000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without view  0: accuracy = 0.8978, drop = +0.0022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without view  1: accuracy = 0.8973, drop = +0.0027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without view  2: accuracy = 0.9005, drop = -0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without view  3: accuracy = 0.9000, drop = +0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without view  4: accuracy = 0.8995, drop = +0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without view  5: accuracy = 0.8989, drop = +0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without view  6: accuracy = 0.8995, drop = +0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without view  7: accuracy = 0.9016, drop = -0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without view  8: accuracy = 0.8984, drop = +0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without view  9: accuracy = 0.8989, drop = +0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without view 10: accuracy = 0.9000, drop = +0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without view 11: accuracy = 0.8978, drop = +0.0022\n",
      "\n",
      "Summary of accuracy drops by view:\n",
      " View  0: Δ = +0.0022\n",
      " View  1: Δ = +0.0027\n",
      " View  2: Δ = -0.0005\n",
      " View  3: Δ = +0.0000\n",
      " View  4: Δ = +0.0005\n",
      " View  5: Δ = +0.0011\n",
      " View  6: Δ = +0.0005\n",
      " View  7: Δ = -0.0016\n",
      " View  8: Δ = +0.0016\n",
      " View  9: Δ = +0.0011\n",
      " View 10: Δ = +0.0000\n",
      " View 11: Δ = +0.0022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "view_indices = list(range(12))  # adjust if you have a different number of views\n",
    "\n",
    "# 1) Baseline object-level accuracy\n",
    "correct_baseline = 0\n",
    "total_objects = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Outer tqdm over batches\n",
    "    for data in tqdm(test_loader, desc='Baseline Evaluation', ncols=80):\n",
    "        imgs, labels = data[1], data[0]           # imgs: [N, V, C, H, W], labels: [N]\n",
    "        N, V, C, H, W = imgs.shape\n",
    "\n",
    "        # Flatten all views\n",
    "        in_all = imgs.view(-1, C, H, W).to(device)       # [N*V, C, H, W]\n",
    "        lbls  = labels.to(device)                        # [N]\n",
    "\n",
    "        out = model(in_all)                              # [N*V, num_classes]\n",
    "        pred = out.argmax(dim=1)                         # [N*V]\n",
    "\n",
    "        # Majority vote per object\n",
    "        for i in range(N):\n",
    "            block = pred[i*V:(i+1)*V]\n",
    "            vote = torch.mode(block.to('cpu'))[0].to(device)\n",
    "            if vote.item() == lbls[i].item():\n",
    "                correct_baseline += 1\n",
    "            total_objects += 1\n",
    "\n",
    "baseline_acc = correct_baseline / total_objects\n",
    "print(f\"\\nBaseline object-level accuracy: {baseline_acc:.4f}\\n\")\n",
    "\n",
    "\n",
    "# 2) Accuracy with each view removed\n",
    "drop_acc = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for v in view_indices:\n",
    "        correct = 0\n",
    "        total   = 0\n",
    "\n",
    "        # Progress bar over batches, indicating which view is removed\n",
    "        desc = f'Remove view {v:2d}'\n",
    "        for data in tqdm(test_loader, desc=desc, ncols=80, leave=False):\n",
    "            imgs, labels = data[1], data[0]        # [N, V, C, H, W], [N]\n",
    "            N, V, C, H, W = imgs.shape\n",
    "\n",
    "            # remove view v across all samples\n",
    "            keep_idx = [i for i in range(V) if i != v]\n",
    "            sub = imgs[:, keep_idx, :, :, :]       # [N, V-1, C, H, W]\n",
    "            in_sub = sub.contiguous().view(-1, C, H, W).to(device)  # [N*(V-1), C, H, W]\n",
    "            lbls  = labels.to(device)\n",
    "\n",
    "            out = model(in_sub)                    # [N*(V-1), num_classes]\n",
    "            pred = out.argmax(dim=1)               # [N*(V-1)]\n",
    "\n",
    "            # Majority vote per object\n",
    "            for i in range(N):\n",
    "                block = pred[i*(V-1):(i+1)*(V-1)]\n",
    "                vote = torch.mode(block.to('cpu'))[0].to(device)\n",
    "                if vote.item() == lbls[i].item():\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "        acc = correct / total\n",
    "        drop_acc[v] = acc\n",
    "        delta = baseline_acc - acc\n",
    "        print(f\"Without view {v:2d}: accuracy = {acc:.4f}, drop = {delta:+.4f}\")\n",
    "\n",
    "# 3) Optionally, summarize all drops\n",
    "print(\"\\nSummary of accuracy drops by view:\")\n",
    "for v in view_indices:\n",
    "    print(f\" View {v:2d}: Δ = {baseline_acc - drop_acc[v]:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100613c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(per_view_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02fb86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_correct_points_baseline/all_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876c0e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for view in per_view_importance:\n",
    "    print(f\"view {view} removed, accuracy now: {per_view_importance[view]['all_correct']*100/per_view_importance[view]['all_points']:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7394b448",
   "metadata": {},
   "outputs": [],
   "source": [
    "for view in per_view_importance:\n",
    "    print(view,\",\" ,sum(per_view_importance[view]) / len(per_view_importance[view])) if per_view_importance[view] else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8bdf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_norm([sum(per_view_importance[view]) for view in per_view_importance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a81733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_l1 = max(sum(per_view_importance[view])/len(per_view_importance[view]) for view in per_view_importance)\n",
    "min_l1 = min(sum(per_view_importance[view])/len(per_view_importance[view]) for view in per_view_importance)\n",
    "min_l1+max_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9800f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_norm(arr):\n",
    "    norm = sum(abs(x) for x in arr)\n",
    "    return [x / (norm) for x in arr] if norm != 0 else arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d52e5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_norm(([sum(per_view_importance[view])/len(per_view_importance[view]) for view in per_view_importance]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baa3aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = [0.08223510806536635,\n",
    "    0.08328940432261465,\n",
    "    0.08389185932675652,\n",
    "    0.08328940432261465,\n",
    "    0.08411777995330974,\n",
    "    0.0835906318246856,\n",
    "    0.0827622561939905,\n",
    "    0.08291286994502597,\n",
    "    0.08374124557572106,\n",
    "    0.0833647111981324,\n",
    "    0.08246102869191957,\n",
    "    0.08434370057986294]\n",
    "norms = [1-min_l1+max_l1-x for x in norms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312823f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b912f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_imp(arr):\n",
    "    mean = sum(abs(x) for x in arr)/len(arr)\n",
    "    return [(mean)/x for x in arr] if mean != 0 else arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374a51ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_imp(norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a1b051",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_imp([sum(per_view_importance[view]) for view in per_view_importance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5a87e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(find_imp([sum(per_view_importance[view]) for view in per_view_importance]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faa8e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(find_imp([sum(per_view_importance[view]) for view in per_view_importance]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c27b4f1",
   "metadata": {},
   "source": [
    "## Baseline Accuarcy of Whole MVCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536980ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ImgDataset.MultiviewImgDataset(\n",
    "    root_dir='../../../MVCNN/ModelNet40-12View/*/test',\n",
    "    scale_aug=False,\n",
    "    rot_aug=False,\n",
    "    test_mode=True,\n",
    "    num_models=1000,\n",
    "    num_views=12,\n",
    "    shuffle=False\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f91d9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Calculate validation accuracy\"\"\"\n",
    "print(\"Calculating validation accuracy...\")\n",
    "model.eval()\n",
    "samples_class = torch.zeros(33)\n",
    "wrong_class = torch.zeros(33)\n",
    "all_points = 0\n",
    "all_correct_points = 0\n",
    "all_loss = 0.0\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    pbar = tqdm(\n",
    "        test_loader, \n",
    "        desc=\"Validating\", \n",
    "        leave=False,\n",
    "        ncols=80\n",
    "    )\n",
    "    for _, data in enumerate(pbar, 0):\n",
    "        N, V, C, H, W = data[1].size()\n",
    "        in_data = data[1].view(-1, C, H, W).to(device)\n",
    "        target = data[0].to(device).repeat_interleave(V)\n",
    "        \n",
    "        out_data = model(in_data)\n",
    "        pred = torch.max(out_data, 1)[1]\n",
    "        all_loss += torch.nn.functional.cross_entropy(out_data, target).item()\n",
    "        results = pred==target\n",
    "        \n",
    "        for i in range(N):\n",
    "            obj_preds = pred[i*V:(i+1)*V]\n",
    "            obj_pred = torch.mode(obj_preds.to('cpu'))[0].to(device)\n",
    "            obj_target = target[i*V]\n",
    "            \n",
    "            if obj_target != obj_pred:\n",
    "                wrong_class[obj_target.item()] += 1\n",
    "            samples_class[obj_target.item()] += 1\n",
    "        \n",
    "        correct = torch.sum(results.long())\n",
    "        all_correct_points += correct.item()\n",
    "        all_points += results.size()[0]\n",
    "\n",
    "print(\"Validation accuracy: \", all_correct_points/all_points)\n",
    "print(\"Validation loss: \", all_loss/len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36703237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
