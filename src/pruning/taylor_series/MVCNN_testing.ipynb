{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3ab051a",
   "metadata": {},
   "source": [
    "# Architecture\n",
    "\n",
    "### SVCNN and MVCNN\n",
    "SVCNN.net_1 is only required. This will output a feature map of (512, 7, 7) after computation. 12 views would be (12, 512, 7, 7) = 25088. \n",
    "\n",
    "Then this is sent to MVCNN. net_1 of MVCNN can be removed as it is unnecessary. \n",
    "\n",
    "So 12xSVCNN (25088) -> MVCNN with an input size of 25088 so thats perfect\n",
    "\n",
    "then MVCNN does the final classification\n",
    "\n",
    "If we are to compute the gradient of each SVCNN, we'll check how much removing each SVCNN affects the loss, then they're ranked. \n",
    "\n",
    "if SVCNN_1 importance is 60%, theoritically we can prune 40% but idk if that's the best option but lets see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d688a85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "parent_dir = os.path.abspath('../../MVCNN')\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef73f6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models import MVCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e399ce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "svcnn_weights = torch.load(\"/Users/mahadevsunilkumar/Desktop/DDNN-Working/src/MVCNN/trained_models/MVCNN_stage_1/MVCNN/model-00006.pth\", map_location='mps', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "cb52e599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svcnn = MVCNN.SVCNN('svcnn')\n",
    "svcnn.load_state_dict(svcnn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dfaf95",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "0cc147dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mvcnn_weights = torch.load('/Users/mahadevsunilkumar/Desktop/DDNN-Working/src/MVCNN/trained_models/MVCNN_stage_2/MVCNN/model-00007.pth', map_location='mps', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e36605ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "mvcnn = MVCNN.MVCNN('mvcnn', svcnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5e5d6d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mvcnn.load_state_dict(mvcnn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a16fdda1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Dropout(p=0.5, inplace=False)\n",
       "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (4): ReLU(inplace=True)\n",
       "  (5): Dropout(p=0.5, inplace=False)\n",
       "  (6): Linear(in_features=4096, out_features=40, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svcnn = svcnn.net_1\n",
    "mvcnn = mvcnn.net_2\n",
    "svcnn, mvcnn = svcnn.to('mps'), mvcnn.to('mps')\n",
    "svcnn.eval()\n",
    "mvcnn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "fbedde5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "svcnn_12 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "be0b8324",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(12):\n",
    "    svcnn_12[f'View_{i}'] = svcnn"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e9c17a4",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "- [ ] Algorithm to find the least important view\n",
    "- [ ] add that in with the pruning workflow\n",
    "- [ ] make that relation curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60d721b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "4cda34ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 6.2666e-01,  1.0340e+00, -1.5818e+00,  ...,  6.2205e-01,\n",
       "            4.9044e-01, -1.4171e+00],\n",
       "          [ 9.0482e-01, -5.8112e-01, -5.4249e-01,  ...,  7.5165e-03,\n",
       "            1.0782e+00, -3.3952e-01],\n",
       "          [ 4.1139e-01,  3.1233e-01, -4.0562e-01,  ..., -6.2831e-01,\n",
       "           -2.5362e-01,  1.2244e+00],\n",
       "          ...,\n",
       "          [-4.9856e-01,  1.0945e+00,  9.5559e-01,  ..., -9.6129e-01,\n",
       "            4.0098e-01,  1.9027e+00],\n",
       "          [-2.9621e-03, -1.2129e+00,  1.5838e+00,  ..., -3.3259e-01,\n",
       "           -2.8019e-01,  7.3189e-01],\n",
       "          [-1.9951e+00,  4.5152e-01,  1.9030e-01,  ..., -8.5170e-02,\n",
       "            2.6114e-02,  8.4217e-02]],\n",
       "\n",
       "         [[ 1.9851e+00,  1.5599e+00,  1.9870e+00,  ..., -1.0362e+00,\n",
       "           -8.4626e-01,  1.6317e+00],\n",
       "          [-6.1954e-01, -1.4328e+00,  1.3768e-01,  ...,  1.9129e-01,\n",
       "           -6.2494e-01,  3.4620e-03],\n",
       "          [-1.1296e+00, -2.2089e-01, -9.9264e-01,  ..., -1.8576e-01,\n",
       "            1.0527e+00, -5.2941e-01],\n",
       "          ...,\n",
       "          [ 5.9681e-01,  3.8953e-01, -1.4493e-01,  ...,  1.3464e+00,\n",
       "           -2.4684e-01, -8.7046e-01],\n",
       "          [ 2.0841e+00, -5.6441e-02,  3.2246e-01,  ..., -2.0290e+00,\n",
       "           -2.6351e-02,  5.6060e-01],\n",
       "          [-8.5155e-01,  3.5271e-01, -3.0144e+00,  ..., -1.1872e+00,\n",
       "            1.8281e-01, -1.8627e+00]],\n",
       "\n",
       "         [[-2.2553e+00,  7.1647e-01,  1.4069e+00,  ..., -5.6546e-01,\n",
       "           -3.5773e-02,  7.3408e-01],\n",
       "          [-2.5303e-02,  7.3547e-01, -3.5486e-01,  ..., -9.2251e-01,\n",
       "           -5.9358e-01, -1.2259e+00],\n",
       "          [ 2.0525e+00, -1.1301e+00, -2.6027e-03,  ..., -2.8155e+00,\n",
       "            6.8468e-02, -1.3431e+00],\n",
       "          ...,\n",
       "          [-8.5636e-01,  2.7343e-01, -2.4607e-01,  ...,  1.5279e-02,\n",
       "            1.6431e+00,  6.2996e-01],\n",
       "          [-6.1385e-02, -5.9036e-01,  2.1165e-01,  ..., -4.5317e-01,\n",
       "           -4.9219e-01, -1.1476e+00],\n",
       "          [-2.8725e-01,  3.2797e-01, -1.3728e-01,  ...,  1.4005e+00,\n",
       "           -7.2493e-01, -2.0312e+00]]]], device='mps:0')"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_svcnn = torch.randn(1, 3, 224, 224).to('mps')\n",
    "x_svcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "2a5dabdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_svcnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "4872e43c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 25088])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tenor_4d = torch.randn(12, 512, 7, 7)\n",
    "max_pooled = torch.max(tenor_4d, 0)[0].to('mps')\n",
    "max_pooled = max_pooled.flatten().unsqueeze(0)\n",
    "max_pooled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c76bd1a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -9.2765,  -6.3426,  -3.5377,  -6.2333, -10.2069, -10.2224,  -7.3638,\n",
       "         -10.3639,  -6.5267,  -7.3924,  -4.9836,  -6.6519,  -2.5672, -12.2452,\n",
       "          -6.6258,  -3.0855,  -8.5797,  -7.9177, -14.1395,  -2.5575,  -4.7683,\n",
       "         -10.2511,  -4.4620,  -2.2969,  -8.0255,  -4.0723,  -3.7950, -10.3833,\n",
       "          -9.7041,  -8.8675,  -9.4005,  -4.8046,  -5.7892,   1.8220,  -5.1677,\n",
       "         -10.6601,  -7.5772,  -3.0590, -12.4178, -11.0302]], device='mps:0',\n",
       "       grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mvcnn(max_pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61e4e36",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
