{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fb85f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import MVCNN\n",
    "from tools import ImgDataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38a14b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0edaa679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVCNN(\n",
       "  (net_1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU(inplace=True)\n",
       "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (net_2): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=33, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MVCNN.SVCNN('svcnn')\n",
    "weights = torch.load('../test_results/MVCNN/trained-models/MVCNN/model-00030.pth', map_location=device)\n",
    "model.load_state_dict(weights)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9140803a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ImgDataset.MultiviewImgDataset(\n",
    "    root_dir='ModelNet40-12View/*/test',\n",
    "    scale_aug=False,\n",
    "    rot_aug=False,\n",
    "    test_mode=True,\n",
    "    num_models=0,\n",
    "    num_views=12,\n",
    "    shuffle=False\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d564299",
   "metadata": {},
   "source": [
    "# View Importance Calculation with Taylor Expansion\n",
    "\n",
    "In multi-view CNN architectures, not all views contribute equally to the final prediction. Some views may capture more discriminative features than others. We'll calculate the importance of each view using a first-order Taylor expansion approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c584877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract CNN feature extractor from the model\n",
    "svcnn = model.net_1\n",
    "num_views = 12  # Number of views per model\n",
    "\n",
    "# Initialize tensor to store view importance values\n",
    "view_importance = torch.zeros(num_views).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b43045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_view_importance(data, target):\n",
    "    \"\"\"\n",
    "    Calculate the importance of each view using first-order Taylor expansion\n",
    "    \n",
    "    Arguments:\n",
    "        data: Input tensor of shape [batch_size, num_views, channels, height, width]\n",
    "        target: Ground truth labels\n",
    "        \n",
    "    Returns:\n",
    "        view_importance: Tensor of shape [num_views] containing importance score for each view\n",
    "    \"\"\"\n",
    "    batch_size = data.shape[0]\n",
    "    num_views = data.shape[1]\n",
    "    importance_scores = torch.zeros(num_views).to(device)\n",
    "    \n",
    "    # Use a smaller batch size if needed to avoid memory issues\n",
    "    sub_batch_size = min(10, batch_size)\n",
    "    \n",
    "    for batch_idx in range(0, batch_size, sub_batch_size):\n",
    "        end_idx = min(batch_idx + sub_batch_size, batch_size)\n",
    "        current_batch_size = end_idx - batch_idx\n",
    "        \n",
    "        # Get data and targets for current sub-batch\n",
    "        sub_data = data[batch_idx:end_idx]\n",
    "        sub_targets = target[batch_idx:end_idx]\n",
    "        \n",
    "        # Process each view separately\n",
    "        view_features = []\n",
    "        for v in range(num_views):\n",
    "            # Ensure we track gradients\n",
    "            view_data = sub_data[:, v].clone().requires_grad_(True)\n",
    "            \n",
    "            # Forward pass through feature extractor\n",
    "            features = svcnn(view_data)\n",
    "            features = features.view(current_batch_size, -1)  # Flatten\n",
    "            view_features.append(features)\n",
    "        \n",
    "        # Process each sample in the sub-batch\n",
    "        for i in range(current_batch_size):\n",
    "            # Extract features for this sample across all views\n",
    "            sample_features = [feat[i:i+1] for feat in view_features]  # Keep batch dimension\n",
    "            \n",
    "            # Perform max pooling across views (MVCNN approach)\n",
    "            pooled_features, indices = torch.max(torch.stack(sample_features, dim=1), dim=1)\n",
    "            \n",
    "            # Pass through classifier\n",
    "            classifier = model.net_2\n",
    "            logits = classifier(pooled_features)\n",
    "            \n",
    "            # Set target score as the one to maximize\n",
    "            sample_target = sub_targets[i].item()\n",
    "            target_score = logits[0, sample_target]  # Score for the correct class\n",
    "            \n",
    "            # Compute gradient of target score w.r.t features\n",
    "            target_score.backward(retain_graph=(i < current_batch_size-1))\n",
    "            \n",
    "            # Calculate importance for each view using Taylor approximation\n",
    "            for v in range(num_views):\n",
    "                if view_data.grad is not None:\n",
    "                    # Taylor importance: |gradient * feature|\n",
    "                    grad = view_data.grad[i].abs()  # Gradient magnitude\n",
    "                    feature = sub_data[i, v].abs()  # Feature magnitude\n",
    "                    importance = (grad * feature).sum().item()\n",
    "                    importance_scores[v] += importance\n",
    "            \n",
    "            # Clear gradients for next iteration\n",
    "            if view_data.grad is not None:\n",
    "                view_data.grad.zero_()\n",
    "    \n",
    "    # Normalize importance scores\n",
    "    importance_sum = importance_scores.sum()\n",
    "    if importance_sum > 0:\n",
    "        importance_scores = importance_scores / importance_sum\n",
    "    \n",
    "    return importance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c196f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def calculate_view_importance(data, target):\n",
    "    \"\"\"\n",
    "    Calculate normalized importance of each view using first-order Taylor approximation\n",
    "\n",
    "    Args:\n",
    "        data (Tensor): [batch_size, num_views, C, H, W] input images\n",
    "        target (Tensor): [batch_size] ground-truth labels\n",
    "        svcnn (nn.Module): feature extractor (first stage of MVCNN)\n",
    "        model (nn.Module): full MVCNN model with attribute `net_2` as classifier\n",
    "        device (torch.device): computation device\n",
    "\n",
    "    Returns:\n",
    "        Tensor: [num_views] normalized importance scores (sum to 1)\n",
    "    \"\"\"\n",
    "    data = data.to(device)\n",
    "    target = target.to(device)\n",
    "\n",
    "    batch_size, num_views = data.shape[0], data.shape[1]\n",
    "    importance_scores = torch.zeros(num_views, device=device)\n",
    "\n",
    "    # Avoid OOM by splitting into sub-batches\n",
    "    sub_batch_size = min(10, batch_size)\n",
    "    for start in range(0, batch_size, sub_batch_size):\n",
    "        end = min(start + sub_batch_size, batch_size)\n",
    "        sb_data = data[start:end]       # [bs, V, C, H, W]\n",
    "        sb_target = target[start:end]   # [bs]\n",
    "        bs = sb_data.size(0)\n",
    "\n",
    "        # Extract features per view with gradient tracking\n",
    "        view_data_list = []\n",
    "        view_feat_list = []\n",
    "        for v in range(num_views):\n",
    "            vd = sb_data[:, v].clone().to(device).requires_grad_(True)\n",
    "            view_data_list.append(vd)\n",
    "            feats = svcnn(vd)                       # [bs, feat_dim, ...]\n",
    "            feats = feats.view(bs, -1)\n",
    "            feats.retain_grad()                     # ensure non-leaf retains grad\n",
    "            view_feat_list.append(feats)\n",
    "\n",
    "        # MVCNN max-pooling across views\n",
    "        feats_stack = torch.stack(view_feat_list, dim=1)  # [bs, V, D]\n",
    "        pooled, _ = feats_stack.max(dim=1)                # [bs, D]\n",
    "\n",
    "        # Classification\n",
    "        logits = model.net_2(pooled)                      # [bs, num_classes]\n",
    "        # Gather scores for correct class\n",
    "        idx = torch.arange(bs, device=device)\n",
    "        sample_scores = logits[idx, sb_target]            # [bs]\n",
    "\n",
    "        # Single backward for all samples\n",
    "        total_score = sample_scores.sum()\n",
    "        total_score.backward()\n",
    "\n",
    "        # Compute and accumulate importance per view\n",
    "        for v in range(num_views):\n",
    "            grad_feat = view_feat_list[v].grad                # [bs, D]\n",
    "            feat = view_feat_list[v]                          # [bs, D]\n",
    "            imp = (grad_feat.abs() * feat.abs()).sum().item()\n",
    "            importance_scores[v] += imp\n",
    "\n",
    "        # Clear gradients for next sub-batch\n",
    "        for vd in view_data_list:\n",
    "            if vd.grad is not None:\n",
    "                vd.grad.zero_()\n",
    "        for vf in view_feat_list:\n",
    "            if vf.grad is not None:\n",
    "                vf.grad.zero_()\n",
    "\n",
    "    # Normalize scores\n",
    "    total = importance_scores.sum()\n",
    "    if total > 0:\n",
    "        importance_scores = importance_scores / total\n",
    "\n",
    "    return importance_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81faf9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Alternative implementation with hook-based approach\n",
    "# def calculate_view_importance_hooks(data, target):\n",
    "#     \"\"\"\n",
    "#     Calculate the importance of each view using first-order Taylor expansion with hooks\n",
    "    \n",
    "#     This implementation uses PyTorch hooks to capture gradients directly\n",
    "#     \"\"\"\n",
    "#     batch_size = min(5, data.shape[0])  # Process a small batch for memory efficiency\n",
    "#     num_views = data.shape[1]\n",
    "\n",
    "#     # Store importance scores\n",
    "#     importance_scores = torch.zeros(num_views).to(device)\n",
    "\n",
    "#     # Helper functions to store activations and gradients\n",
    "#     activations = {}\n",
    "#     gradients = {}\n",
    "\n",
    "#     def save_activation(name):\n",
    "#         def hook(module, input, output):\n",
    "#             activations[name] = output\n",
    "#         return hook\n",
    "\n",
    "#     def save_gradient(name):\n",
    "#         def hook(module, grad_input, grad_output):\n",
    "#             gradients[name] = grad_output[0]\n",
    "#         return hook\n",
    "\n",
    "#     # Register hooks on the model\n",
    "#     hooks = []\n",
    "#     for i, module in enumerate(model.net_1):\n",
    "#         if isinstance(module, torch.nn.Conv2d) and i in [0, 3, 6, 8, 11, 13, 16, 18]:\n",
    "#             hooks.append(module.register_forward_hook(save_activation(f'conv{i}')))\n",
    "#             hooks.append(module.register_backward_hook(save_gradient(f'conv{i}')))\n",
    "#\n",
    "#     # Process each sample to calculate view importance\n",
    "#     for i in range(batch_size):\n",
    "#         for v in range(num_views):\n",
    "#             # Process a single view\n",
    "#             view_data = data[i:i+1, v].to(device)\n",
    "\n",
    "#             # Forward pass\n",
    "#             features = svcnn(view_data)\n",
    "#             features = features.view(1, -1)  # Flatten\n",
    "\n",
    "#             # Pass through classifier\n",
    "#             logits = model.net_2(features)\n",
    "\n",
    "#             # Target score\n",
    "#             sample_target = target[i].item()\n",
    "#             target_score = logits[0, sample_target]\n",
    "\n",
    "#             # Backward pass\n",
    "#             model.zero_grad()\n",
    "#             target_score.backward()\n",
    "\n",
    "#             # Calculate importance using Taylor approximation\n",
    "#             for key in activations:\n",
    "#                 if key in gradients:\n",
    "#                     # Taylor importance: |gradient * activation|\n",
    "#                     act = activations[key]\n",
    "#                     grad = gradients[key]\n",
    "#                     importance = torch.sum(torch.abs(act * grad)).item()\n",
    "#                     importance_scores[v] += importance / len(activations)\n",
    "\n",
    "#     # Clean up hooks\n",
    "#     for hook in hooks:\n",
    "#         hook.remove()\n",
    "\n",
    "#     # Normalize importance scores\n",
    "#     if torch.sum(importance_scores) > 0:\n",
    "#         importance_scores = importance_scores / torch.sum(importance_scores)\n",
    "\n",
    "#     return importance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb954618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple approach for view importance\n",
    "def simple_view_importance(data, target):\n",
    "    \"\"\"A simpler approach that uses feature magnitudes directly\"\"\"\n",
    "    num_samples = min(20, data.shape[0])\n",
    "    num_views = data.shape[1]\n",
    "    importance_scores = torch.zeros(num_views).to(device)\n",
    "    \n",
    "    # Model components\n",
    "    feature_extractor = model.net_1\n",
    "    classifier = model.net_2\n",
    "    \n",
    "    # Get feature contributions\n",
    "    for i in range(num_samples):\n",
    "        # Get sample data and target\n",
    "        sample_data = data[i:i+1]\n",
    "        sample_target = target[i:i+1]\n",
    "        \n",
    "        # Extract features for each view\n",
    "        view_features = []\n",
    "        for v in range(num_views):\n",
    "            view_data = sample_data[:, v]\n",
    "            with torch.no_grad():\n",
    "                features = feature_extractor(view_data)\n",
    "                features = features.view(1, -1)\n",
    "                view_features.append(features)\n",
    "                \n",
    "        # For each view, calculate importance\n",
    "        for v in range(num_views):\n",
    "            # Create a list of all views but with this view replaced with zeros\n",
    "            mask_features = view_features.copy()\n",
    "            mask_features[v] = torch.zeros_like(mask_features[v])\n",
    "            \n",
    "            # Max pooling across views\n",
    "            with torch.no_grad():\n",
    "                pooled, _ = torch.max(torch.cat(mask_features, dim=0), dim=0, keepdim=True)\n",
    "                masked_output = classifier(pooled)\n",
    "                \n",
    "                # Calculate full output with all views\n",
    "                full_pooled, _ = torch.max(torch.cat(view_features, dim=0), dim=0, keepdim=True)\n",
    "                full_output = classifier(full_pooled)\n",
    "                \n",
    "                # Importance is how much output changes when view is masked\n",
    "                target_idx = sample_target.item()\n",
    "                importance = (full_output[0, target_idx] - masked_output[0, target_idx]).abs()\n",
    "                importance_scores[v] += importance.item()\n",
    "    \n",
    "    # Normalize\n",
    "    if importance_scores.sum() > 0:\n",
    "        importance_scores = importance_scores / importance_scores.sum()\n",
    "        \n",
    "    return importance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cbe663f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating view importance...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/233\n",
      "Processing batch 2/233\n",
      "Processing batch 3/233\n",
      "Processing batch 4/233\n",
      "Processing batch 5/233\n",
      "Processing batch 6/233\n",
      "Processing batch 7/233\n",
      "Processing batch 8/233\n",
      "Processing batch 9/233\n",
      "Processing batch 10/233\n",
      "Processing batch 11/233\n",
      "Processing batch 12/233\n",
      "Processing batch 13/233\n",
      "Processing batch 14/233\n",
      "Processing batch 15/233\n",
      "Processing batch 16/233\n",
      "Processing batch 17/233\n",
      "Processing batch 18/233\n",
      "Processing batch 19/233\n",
      "Processing batch 20/233\n",
      "Processing batch 21/233\n",
      "Processing batch 22/233\n",
      "Processing batch 23/233\n",
      "Processing batch 24/233\n",
      "Processing batch 25/233\n",
      "Processing batch 26/233\n",
      "Processing batch 27/233\n",
      "Processing batch 28/233\n",
      "Processing batch 29/233\n",
      "Processing batch 30/233\n",
      "Processing batch 31/233\n",
      "Processing batch 32/233\n",
      "Processing batch 33/233\n",
      "Processing batch 34/233\n",
      "Processing batch 35/233\n",
      "Processing batch 36/233\n",
      "Processing batch 37/233\n",
      "Processing batch 38/233\n",
      "Processing batch 39/233\n",
      "Processing batch 40/233\n",
      "Processing batch 41/233\n",
      "Processing batch 42/233\n",
      "Processing batch 43/233\n",
      "Processing batch 44/233\n",
      "Processing batch 45/233\n",
      "Processing batch 46/233\n",
      "Processing batch 47/233\n",
      "Processing batch 48/233\n",
      "Processing batch 49/233\n",
      "Processing batch 50/233\n",
      "Processing batch 51/233\n",
      "Processing batch 52/233\n",
      "Processing batch 53/233\n",
      "Processing batch 54/233\n",
      "Processing batch 55/233\n",
      "Processing batch 56/233\n",
      "Processing batch 57/233\n",
      "Processing batch 58/233\n",
      "Processing batch 59/233\n",
      "Processing batch 60/233\n",
      "Processing batch 61/233\n",
      "Processing batch 62/233\n",
      "Processing batch 63/233\n",
      "Processing batch 64/233\n",
      "Processing batch 65/233\n",
      "Processing batch 66/233\n",
      "Processing batch 67/233\n",
      "Processing batch 68/233\n",
      "Processing batch 69/233\n",
      "Processing batch 70/233\n",
      "Processing batch 71/233\n",
      "Processing batch 72/233\n",
      "Processing batch 73/233\n",
      "Processing batch 74/233\n",
      "Processing batch 75/233\n",
      "Processing batch 76/233\n",
      "Processing batch 77/233\n",
      "Processing batch 78/233\n",
      "Processing batch 79/233\n",
      "Processing batch 80/233\n",
      "Processing batch 81/233\n",
      "Processing batch 82/233\n",
      "Processing batch 83/233\n",
      "Processing batch 84/233\n",
      "Processing batch 85/233\n",
      "Processing batch 86/233\n",
      "Processing batch 87/233\n",
      "Processing batch 88/233\n",
      "Processing batch 89/233\n",
      "Processing batch 90/233\n",
      "Processing batch 91/233\n",
      "Processing batch 92/233\n",
      "Processing batch 93/233\n",
      "Processing batch 94/233\n",
      "Processing batch 95/233\n",
      "Processing batch 96/233\n",
      "Processing batch 97/233\n",
      "Processing batch 98/233\n",
      "Processing batch 99/233\n",
      "Processing batch 100/233\n",
      "Processing batch 101/233\n",
      "Processing batch 102/233\n",
      "Processing batch 103/233\n",
      "Processing batch 104/233\n",
      "Processing batch 105/233\n",
      "Processing batch 106/233\n",
      "Processing batch 107/233\n",
      "Processing batch 108/233\n",
      "Processing batch 109/233\n",
      "Processing batch 110/233\n",
      "Processing batch 111/233\n",
      "Processing batch 112/233\n",
      "Processing batch 113/233\n",
      "Processing batch 114/233\n",
      "Processing batch 115/233\n",
      "Processing batch 116/233\n",
      "Processing batch 117/233\n",
      "Processing batch 118/233\n",
      "Processing batch 119/233\n",
      "Processing batch 120/233\n",
      "Processing batch 121/233\n",
      "Processing batch 122/233\n",
      "Processing batch 123/233\n",
      "Processing batch 124/233\n",
      "Processing batch 125/233\n",
      "Processing batch 126/233\n",
      "Processing batch 127/233\n",
      "Processing batch 128/233\n",
      "Processing batch 129/233\n",
      "Processing batch 130/233\n",
      "Processing batch 131/233\n",
      "Processing batch 132/233\n",
      "Processing batch 133/233\n",
      "Processing batch 134/233\n",
      "Processing batch 135/233\n",
      "Processing batch 136/233\n",
      "Processing batch 137/233\n",
      "Processing batch 138/233\n",
      "Processing batch 139/233\n",
      "Processing batch 140/233\n",
      "Processing batch 141/233\n",
      "Processing batch 142/233\n",
      "Processing batch 143/233\n",
      "Processing batch 144/233\n",
      "Processing batch 145/233\n",
      "Processing batch 146/233\n",
      "Processing batch 147/233\n",
      "Processing batch 148/233\n",
      "Processing batch 149/233\n",
      "Processing batch 150/233\n",
      "Processing batch 151/233\n",
      "Processing batch 152/233\n",
      "Processing batch 153/233\n",
      "Processing batch 154/233\n",
      "Processing batch 155/233\n",
      "Processing batch 156/233\n",
      "Processing batch 157/233\n",
      "Processing batch 158/233\n",
      "Processing batch 159/233\n",
      "Processing batch 160/233\n",
      "Processing batch 161/233\n",
      "Processing batch 162/233\n",
      "Processing batch 163/233\n",
      "Processing batch 164/233\n",
      "Processing batch 165/233\n",
      "Processing batch 166/233\n",
      "Processing batch 167/233\n",
      "Processing batch 168/233\n",
      "Processing batch 169/233\n",
      "Processing batch 170/233\n",
      "Processing batch 171/233\n",
      "Processing batch 172/233\n",
      "Processing batch 173/233\n",
      "Processing batch 174/233\n",
      "Processing batch 175/233\n",
      "Processing batch 176/233\n",
      "Processing batch 177/233\n",
      "Processing batch 178/233\n",
      "Processing batch 179/233\n",
      "Processing batch 180/233\n",
      "Processing batch 181/233\n",
      "Processing batch 182/233\n",
      "Processing batch 183/233\n",
      "Processing batch 184/233\n",
      "Processing batch 185/233\n",
      "Processing batch 186/233\n",
      "Processing batch 187/233\n",
      "Processing batch 188/233\n",
      "Processing batch 189/233\n",
      "Processing batch 190/233\n",
      "Processing batch 191/233\n",
      "Processing batch 192/233\n",
      "Processing batch 193/233\n",
      "Processing batch 194/233\n",
      "Processing batch 195/233\n",
      "Processing batch 196/233\n",
      "Processing batch 197/233\n",
      "Processing batch 198/233\n",
      "Processing batch 199/233\n",
      "Processing batch 200/233\n",
      "Processing batch 201/233\n",
      "Processing batch 202/233\n",
      "Processing batch 203/233\n",
      "Processing batch 204/233\n",
      "Processing batch 205/233\n",
      "Processing batch 206/233\n",
      "Processing batch 207/233\n",
      "Processing batch 208/233\n",
      "Processing batch 209/233\n",
      "Processing batch 210/233\n",
      "Processing batch 211/233\n",
      "Processing batch 212/233\n",
      "Processing batch 213/233\n",
      "Processing batch 214/233\n",
      "Processing batch 215/233\n",
      "Processing batch 216/233\n",
      "Processing batch 217/233\n",
      "Processing batch 218/233\n",
      "Processing batch 219/233\n",
      "Processing batch 220/233\n",
      "Processing batch 221/233\n",
      "Processing batch 222/233\n",
      "Processing batch 223/233\n",
      "Processing batch 224/233\n",
      "Processing batch 225/233\n",
      "Processing batch 226/233\n",
      "Processing batch 227/233\n",
      "Processing batch 228/233\n",
      "Processing batch 229/233\n",
      "Processing batch 230/233\n",
      "Processing batch 231/233\n",
      "Processing batch 232/233\n",
      "Processing batch 233/233\n",
      "\n",
      "View Importance Scores:\n",
      "View 0: 18.52773666381836\n",
      "View 1: 19.941801071166992\n",
      "View 2: 20.061664581298828\n",
      "View 3: 18.424055099487305\n",
      "View 4: 19.111942291259766\n",
      "View 5: 21.318603515625\n",
      "View 6: 20.724056243896484\n",
      "View 7: 17.942764282226562\n",
      "View 8: 18.432174682617188\n",
      "View 9: 18.983192443847656\n",
      "View 10: 18.695262908935547\n",
      "View 11: 20.836746215820312\n"
     ]
    }
   ],
   "source": [
    "view_importance = torch.zeros(num_views).to(device)\n",
    "\n",
    "print(\"Calculating view importance...\")\n",
    "for i, (target, data, paths) in enumerate(test_loader):\n",
    "    print(f\"Processing batch {i+1}/{len(test_loader)}\") \n",
    "    data = data.to(device)\n",
    "    target = target.to(device)\n",
    "    \n",
    "    # Calculate importance for this batch\n",
    "    batch_importance = calculate_view_importance(data, target)\n",
    "    view_importance += batch_importance\n",
    "\n",
    "# # Normalize the accumulated importance scores\n",
    "# if view_importance.sum() > 0:\n",
    "#     view_importance = view_importance / view_importance.sum()\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nView Importance Scores:\")\n",
    "for i, score in enumerate(view_importance):\n",
    "    print(f\"View {i}: {score.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0d3a3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Views sorted by importance (most important first):\n",
      "Rank 1: View 5 - Score 21.318604\n",
      "Rank 2: View 11 - Score 20.836746\n",
      "Rank 3: View 6 - Score 20.724056\n",
      "Rank 4: View 2 - Score 20.061665\n",
      "Rank 5: View 1 - Score 19.941801\n",
      "Rank 6: View 4 - Score 19.111942\n",
      "Rank 7: View 9 - Score 18.983192\n",
      "Rank 8: View 10 - Score 18.695263\n",
      "Rank 9: View 0 - Score 18.527737\n",
      "Rank 10: View 8 - Score 18.432175\n",
      "Rank 11: View 3 - Score 18.424055\n",
      "Rank 12: View 7 - Score 17.942764\n"
     ]
    }
   ],
   "source": [
    "# Sort views by importance\n",
    "sorted_paths = torch.argsort(view_importance, descending=True)\n",
    "print(\"Views sorted by importance (most important first):\")\n",
    "for i, idx in enumerate(sorted_paths):\n",
    "    print(f\"Rank {i+1}: View {idx.item()} - Score {view_importance[idx].item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2be7cf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view_importance = torch.zeros(num_views).to(device)\n",
    "\n",
    "# print(\"Calculating view importance...\")\n",
    "# for i, (target, data, paths) in enumerate(test_loader):\n",
    "#     print(f\"Processing batch {i+1}/{len(test_loader)}\") \n",
    "#     data = data.to(device)\n",
    "#     target = target.to(device)\n",
    "    \n",
    "#     # Calculate importance for this batch\n",
    "#     batch_importance = simple_view_importance(data, target)\n",
    "#     view_importance += batch_importance\n",
    "\n",
    "# # Normalize the accumulated importance scores\n",
    "# # if view_importance.sum() > 0:\n",
    "# #     view_importance = view_importance / view_importance.sum()\n",
    "\n",
    "# # Display the results\n",
    "# print(\"\\nView Importance Scores:\")\n",
    "# for i, score in enumerate(view_importance):\n",
    "#     print(f\"View {i}: {score.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b85ff43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sort views by importance\n",
    "# sorted_paths = torch.argsort(view_importance, descending=True)\n",
    "# print(\"Views sorted by importance (most important first):\")\n",
    "# for i, idx in enumerate(sorted_paths):\n",
    "#     print(f\"Rank {i+1}: View {idx.item()} - Score {view_importance[idx].item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13e0dce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('sorted_view_importance_scores.csv', 'w', newline='') as csvfile:\n",
    "#     fieldnames = ['Rank', 'View', 'Importance Score']\n",
    "#     writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    \n",
    "#     writer.writeheader()\n",
    "#     for i, idx in enumerate(sorted_paths):\n",
    "#         writer.writerow({'Rank': i+1, 'View': idx.item(), 'Importance Score': view_importance[idx].item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d805fbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #normalize the importance scores\n",
    "# view_importance = view_importance / torch.sum(view_importance)\n",
    "# print(\"Normalized View Importance Scores:\")\n",
    "# for i, score in enumerate(view_importance):\n",
    "#     print(f\"View {i}: {score.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d0f1caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize the importance scores\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.style.use('default')\n",
    "# plt.figure(figsize=(15, 6), dpi=400)\n",
    "# plt.bar(range(num_views), view_importance.cpu().numpy(), color='green', alpha=0.7, label='Importance Score', width=0.3)\n",
    "# plt.xlabel('View Index')\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.ylabel('Importance Score')\n",
    "# plt.title('View Importance Based on Taylor Expansion')\n",
    "# plt.xticks(range(num_views))\n",
    "# plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c5ece1",
   "metadata": {},
   "source": [
    "## Load and Display Sample Views\n",
    "\n",
    "Let's visualize some of the most and least important views to understand what the model finds useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "315eeefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import torchvision.transforms as transforms\n",
    "\n",
    "# # Get the first sample from the dataset\n",
    "# for target, data, paths in test_loader:\n",
    "#     sample_paths = paths\n",
    "#     break\n",
    "\n",
    "# # Display most important views versus least important views\n",
    "# plt.figure(figsize=(15, 10))\n",
    "\n",
    "# # Get top 3 and bottom 3 views\n",
    "# top_views = sorted_paths[:3]\n",
    "# bottom_views = sorted_paths[-3:]\n",
    "\n",
    "# # Display top views\n",
    "# for i, idx in enumerate(top_views):\n",
    "#     plt.subplot(2, 3, i+1)\n",
    "#     img_path = sample_paths[idx.item()][0]  # Get the first sample's path for this view\n",
    "#     img = Image.open(img_path)\n",
    "#     plt.imshow(img)\n",
    "#     plt.title(f\"Top {i+1}: View {idx.item()}\\nScore: {view_importance[idx].item():.4f}\")\n",
    "#     plt.axis('off')\n",
    "\n",
    "# # Display bottom views\n",
    "# for i, idx in enumerate(bottom_views):\n",
    "#     plt.subplot(2, 3, i+4)\n",
    "#     img_path = sample_paths[idx.item()][0]  # Get the first sample's path for this view\n",
    "#     img = Image.open(img_path)\n",
    "#     plt.imshow(img)\n",
    "#     plt.title(f\"Bottom {len(bottom_views)-i}: View {idx.item()}\\nScore: {view_importance[idx].item():.4f}\")\n",
    "#     plt.axis('off')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4626efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the view importance scores\n",
    "# import pickle\n",
    "\n",
    "# result = {\n",
    "#     'view_importance': view_importance.cpu().numpy(),\n",
    "#     'sorted_views': sorted_paths.cpu().numpy()\n",
    "# }\n",
    "\n",
    "# with open('view_importance_results.pkl', 'wb') as f:\n",
    "#     pickle.dump(result, f)\n",
    "\n",
    "# print(\"View importance results saved to 'view_importance_results.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39a0456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute baseline accuracy and per-view removal impact\n",
    "# model.eval()\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# with torch.no_grad():\n",
    "#     for target, data, paths in test_loader:\n",
    "#         data, target = data.to(device), target.to(device)\n",
    "#         # extract features for all views and do max-pool\n",
    "#         features = []\n",
    "#         for v in range(num_views):\n",
    "#             f = svcnn(data[:, v])\n",
    "#             features.append(f.view(data.size(0), -1))\n",
    "#         pooled, _ = torch.max(torch.stack(features, dim=1), dim=1)\n",
    "#         outputs = model.net_2(pooled)\n",
    "#         _, pred = outputs.max(dim=1)\n",
    "#         correct += pred.eq(target).sum().item()\n",
    "#         total += target.size(0)\n",
    "# baseline_acc = correct / total\n",
    "# print(f\"Baseline accuracy (all views): {baseline_acc:.4f}\")\n",
    "\n",
    "# # Now measure accuracy when each view is removed\n",
    "# for v in range(num_views):\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for target, data, paths in test_loader:\n",
    "#             data, target = data.to(device), target.to(device)\n",
    "#             data_masked = data.clone()\n",
    "#             data_masked[:, v] = 0  # remove view v\n",
    "#             features = []\n",
    "#             for i in range(num_views):\n",
    "#                 f = svcnn(data_masked[:, i])\n",
    "#                 features.append(f.view(data_masked.size(0), -1))\n",
    "#             pooled, _ = torch.max(torch.stack(features, dim=1), dim=1)\n",
    "#             outputs = model.net_2(pooled)\n",
    "#             _, pred = outputs.max(dim=1)\n",
    "#             correct += pred.eq(target).sum().item()\n",
    "#             total += target.size(0)\n",
    "#     acc = correct / total\n",
    "#     diff = acc - baseline_acc\n",
    "#     print(f\"View {v} removed -> accuracy: {acc:.4f} (Î” {diff:+.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809ace4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
